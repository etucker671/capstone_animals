---
title: "edX-Harvard Data Science Capstone Project: Shelter Animal Outcomes"
author: "Eric Tucker"
date: "March 3rd, 2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = FALSE)

#define project libraries
package.names <- c("tidyverse","lubridate","randomForest","caret")

#define check function
package.check <- function(package.names){
  for(i in 1:length(package.names)){
    if(package.names[i] %in% installed.packages()){
      library(package.names[i], character.only = TRUE)
      cat(package.names[i],"already installed. Library reloaded.\n")
    }
    else{
      install.packages(package.names[i], character.only = TRUE)
      library(package.names[i], character.only = TRUE)
      cat("Missing package",package.names[i],"has been installed.\n")
    }
  }
  cat("All requested libraries have been loaded.\n")
}

#run check function on libraries
package.check(package.names)
```

## Introduction

Every year, approximately 1.5 million animals are euthanized in shelters across the United States. Some of these deaths may be preventable by identifying particularly at risk animals and allowing shelters to focus their budgets towards additional medical procedures, marketing, or play time with potential owners. Austin Animal Center recently released 3 years worth of intake and outcome data on its animals in hopes of doing just this. Here we analyzed these data in an attempt to model the likelihood of euthanization for any incoming animal so that funds can be allocated appropriately and unneccesary deaths can be avoided.


## Data Cleaning, Exploration, and Feature Transformation

The data was first downloaded and imported from its raw CSV format, the first 10 rows of which are shown below. On each animal, we were provided with several predictors including age, sex, spayed/neutered status, breed, color, outcome, outcome subtype, and the date and time of the outcome.

```{r a}
data <- read.csv(url("https://raw.githubusercontent.com/etucker671/capstone_animals/main/animaloutcomes.csv"), header = TRUE)

data %>% head(n=10)
```

The outcome subtype was ignored, as this information is not provided until the outcome is determined, so it could not be used as a predictor. Prior to splitting the data set into a training and validation set, the original outcomes, shown in the table below, were condensed into a binary outcome (Euthanized or Not Euthanized), also shown below.

```{r b}
#print original outcomes
cat("\nOriginal Outcomes:\n")
data$OutcomeType %>% table()

#binarize outcomes
data <- data %>% mutate(Euthanized = OutcomeType == "Euthanasia")

#print new outcomes
cat("\nBinary Outcomes:\n")
data$Euthanized %>% table()
```

This transformation simplied the classification problem and ensured that similar ratios of euthanized vs. non-euthanized pets would be split into the validation and training sets. Once transformed, the data was then split into training and validation sets at an 80:20 ratio.

```{r c}
#split data
if(version$year >= 2019) {
  set.seed(1, sample.kind="Rounding")
} else {
  set.seed(1)
}
index <- createDataPartition(data$Euthanized, p = 0.8, list = FALSE)
full_data <- data
data <- full_data[index,]
val_data <- full_data[-index,]

#clean up
rm(index)
```


We treated the validation data as if it were future observations on which predictions need to be made, and so all data exploration was carried out on only the training data set. However, any cleaning and transformations carried out as a result of the exploratory findings were applied to both data sets in parallel.


### Age Effects

First, we looked at age effects. Before being able to assess these, however, we had to standardize the entries, as they were reported as character strings such as:
```{r e}
data$AgeuponOutcome %>% head()
```

and in a variety of units, including:

```{r ee}
#create conversion dataframe
ages <- data.frame(matrix(ncol = 4, nrow = nrow(data)))
x <- c("number", "unit_orig", "unit_new", "age_years")
colnames(ages) <- x
rm(x)

#Extract numbers
ages$number <- as.numeric(str_extract(data$AgeuponOutcome, "\\d+"))

#Extract units
ages$unit_orig <- str_extract(data$AgeuponOutcome, "[a-z]+")

#Check which units exist
unique(na.omit(ages$unit_orig)) %>% sort()
```


The numbers were extracted from the strings, and the "s" was removed from all plural entries to convert everything to singular units. The extracted numbers were then divided by an appropriate conversion factor to convert everything into years. (Month values were divided by 12, weeks divided by 52, and days divided by 365.) A new feature, AgeInYears, was then created to store these values, the distribution of which is shown below.

```{r f}
#remove "s" to consolidate units
ages$unit_new <- str_remove(ages$unit_orig,"s")

#Produce all values in years
ages <- ages %>% mutate(age_years = ifelse(unit_new == "day", number/365, ifelse(unit_new == "month", number/12, ifelse(unit_new == "week", number/52,number))))

#merge into data
data <- cbind(data,AgeInYears = ages$age_years)

#clean up
rm(ages)

#create conversion dataframe
ages <- data.frame(matrix(ncol = 4, nrow = nrow(val_data)))
x <- c("number", "unit_orig", "unit_new", "age_years")
colnames(ages) <- x
rm(x)

#Extract numbers
ages$number <- as.numeric(str_extract(val_data$AgeuponOutcome, "\\d+"))

#Extract units
ages$unit_orig <- str_extract(val_data$AgeuponOutcome, "[a-z]+")

#Check which units exist
table(ages$unit_orig)

#remove "s" to consolidate units
ages$unit_new <- str_remove(ages$unit_orig,"s")

#Check new units
table(ages$unit_new)

#Produce all values in years
ages <- ages %>% mutate(age_years = ifelse(unit_new == "day", number/365, ifelse(unit_new == "month", number/12, ifelse(unit_new == "week", number/52,number))))

#merge into data
val_data <- cbind(val_data,AgeInYears = ages$age_years)

#clean up
rm(ages)

#print distribution
hist(data$AgeInYears, xlab = "Age In Years", main = "Distribution of Ages in Training Data")
```


We then looked to see if there were age differences between euthanized and non-euthanized animals, and we indeed found some, with statistical significance confirmed by t-test as shown below.

```{r g}
#age distributions of euthanized vs. not
data %>% ggplot(aes(x = Euthanized, y = AgeInYears)) + geom_boxplot()

#t test for statistical age difference between euthanized and non-euthanized groups
t.test(pull(filter(data, Euthanized == TRUE),AgeInYears),pull(filter(data, Euthanized == FALSE),AgeInYears))
```

Given these findings, we kept AgeInYears as a feature to carry forward into modeling attempts.


### Animal Type Effects

There were only two types of animals in this data set: Dogs and Cats. We checked to see if there was any difference in euthanization rates between them, and indeed found there was a small but significant difference, as confirmed by a Chi Squared test (shown below), where cats were euthanized slightly more frequently than dogs (6.4% vs 5.4%). AnimalType was therefore carried forward into the models.

```{r h}
#euthanization rates by animal type
data %>% group_by(AnimalType) %>% summarize(n = n(), euth_prop = mean(Euthanized == TRUE)) %>% arrange(-euth_prop)

#animal type
chisq.test(data$AnimalType,data$Euthanized)
```


### Sex and Fixed Status Effects

The sex information was provided as string, combined with the spayed/neutered status of each animal. Each combination is shown below along with the euthanization proportions observed in the training set, ordered by most frequently to least frequently euthanized.

```{r i}
#euthanization rates by SexUponOutcome
data %>% group_by(SexuponOutcome) %>% summarize(n = n(), euth_prop = mean(Euthanized == TRUE)) %>% arrange(-euth_prop)
```

As shown, males are generally more frequently euthanized than females, and "intact" animals are euthanized more than spayed or neutered animals. We separated out these variables with regex extraction, labeling the separate variables as Sex and, though not an flattering term, "Fixed" Status - representing either neutered (if male) or spayed (if female). Chi Squared tests were run to confirm statistical significance of euthanization rates between each variable, the results of which, along with the corresponding rates, are shown below.

```{r ii}
#extract sex and spay/neuter status
temp <- separate(data[7], col = "SexuponOutcome", into = c("FixedStatus","Sex"), sep = " ")

#reassign spayed/neutered status to general value
temp[temp$FixedStatus=="Spayed",1] <- "Spayed-Neutered"
temp[temp$FixedStatus=="Neutered",1] <- "Spayed-Neutered"

#convert blanks to Unknown
temp[temp$FixedStatus=="",1] <- "Unknown"
temp[is.na(temp$Sex),2] <- "Unknown"

#convert to factors
temp$FixedStatus <- factor(temp$FixedStatus, levels = c("Spayed-Neutered", "Intact", "Unknown"))
temp$Sex <- factor(temp$Sex)

#add to data set
data <- cbind(data,temp)

#clean up
rm(temp)


#### REPEAT FOR VALIDATION SET ####

#extract sex and spay/neuter status
temp <- separate(val_data[7], col = "SexuponOutcome", into = c("FixedStatus","Sex"), sep = " ")

#reassign spayed/neutered status to general value
temp[temp$FixedStatus=="Spayed",1] <- "Spayed-Neutered"
temp[temp$FixedStatus=="Neutered",1] <- "Spayed-Neutered"

#convert blanks to Unknown
temp[temp$FixedStatus=="",1] <- "Unknown"
temp[is.na(temp$Sex),2] <- "Unknown"

#convert to factors
temp$FixedStatus <- factor(temp$FixedStatus, levels = c("Spayed-Neutered", "Intact", "Unknown"))
temp$Sex <- factor(temp$Sex)

#add to data set
val_data <- cbind(val_data,temp)

#clean up
rm(temp)

#euthanization rates by sex
data %>% group_by(Sex) %>% summarize(n = n(), euth_prop = mean(Euthanized == TRUE)) %>% arrange(-euth_prop)

#sex chisq
chisq.test(data$Sex,data$Euthanized)

#euthanization rates by fixed status
data %>% group_by(FixedStatus) %>% summarize(n = n(), euth_prop = mean(Euthanized == TRUE)) %>% arrange(-euth_prop)

#fixed status chisq
chisq.test(data$FixedStatus,data$Euthanized)

```

Through this separation, we saw that unknown sex animals were most likely to be euthanized, followed by males, followed by females (a slightly different order than when viewing the combined variables). The FixedStatus variable followed the same trends as previously observed. Both of these variables were carried forward into the models.


### Seasonal Effects

We reasoned that shelters would be likely to experience variability in both intake and adoption rates throughout the year, so we looked for any seasonal effects on euthanization rates. The exact meaning of the DateTime variable was not made clear by the publishers, so it could either correspond to the intake time or the outcome time. In either case, it was interpreted as an approximate activity time during which an animal was present in the shelter and would arrive at some outcome. We extracted the month from each timestamp and viewed euthanization rates for each, shown below.

```{r j}
#extract month
data <- data %>% mutate(Month = month(DateTime, label = TRUE))
#remove ordering
data$Month <- factor(data$Month, ordered = FALSE)

#### REPEAT FOR VALIDATION DATA ####

#extract month
val_data <- val_data %>% mutate(Month = month(DateTime, label = TRUE))
#remove ordering
val_data$Month <- factor(val_data$Month, ordered = FALSE)

#euth_prop plotted
data %>% group_by(Month) %>% summarize(n = n(), euth_prop = mean(Euthanized == TRUE)) %>% ggplot(aes(x = Month, y = euth_prop)) + geom_point() + geom_line(aes(group=1), col = "red") + ylab("Proportion of Animals Euthanized")

```

It appeared that euthanization rates spiked in the Spring (March through June). To get a clearer picture of what was happening, we looked at both euthanization and adoption counts by month, and saw that they both followed similar but not identical trends (shown below), generally dropping in Winter and Spring but rising in the Fall.

```{r k}
#plotted adoptions by month
data %>% group_by(Month) %>% summarize(Adoptions = sum(OutcomeType == "Adoption"), Euthanizations = sum(OutcomeType == "Euthanasia")) %>% ggplot(aes(x = Month, y = Adoptions)) + geom_point() + geom_line(aes(group=1), col = "blue")

#plotted euthanizations by month
data %>% group_by(Month) %>% summarize(Adoptions = sum(OutcomeType == "Adoption"), Euthanizations = sum(OutcomeType == "Euthanasia")) %>% ggplot(aes(x = Month, y = Euthanizations)) + geom_point() + geom_line(aes(group=1), col = "red")

```


Given the differences in scale for adoptions vs. euthanizations by month, we instead compared them by plotting as percentage deviations from their respective averages (shown below), and we were able to show that although euthanization counts dropped below average in the months of March and April, they did not drop nearly as much as the adoptions, and so the chances of an animal getting euthanized during those months was indeed higher. In April and May, the euthanization counts actually increased above average, while the adoption counts fell below. Collectively, these months were labeled as "Danger Months", and we coded a matching binary variable set to a new feature called "Month Status", the two statuses being "Danger" or "Neutral". In an effort to reduce dimensionality, the MonthStatus variable, not the Month itself, was carried forward into the modeling process.

```{r l}
#plotted as deviations from average
AveAdoptionsPerMonth <- data %>% group_by(Month) %>% 
  summarize(Adoptions = sum(OutcomeType == "Adoption"), Euthanizations = sum(OutcomeType == "Euthanasia")) %>% 
  pull(Adoptions) %>% mean()

AveEuthanizationsPerMonth <- data %>% group_by(Month) %>% 
  summarize(Adoptions = sum(OutcomeType == "Adoption"), Euthanizations = sum(OutcomeType == "Euthanasia")) %>% 
  pull(Euthanizations) %>% mean()

data %>% group_by(Month) %>% 
  summarize(Adoptions = (sum(OutcomeType == "Adoption")-AveAdoptionsPerMonth)/AveAdoptionsPerMonth, 
            Euthanizations = (sum(OutcomeType == "Euthanasia")-AveEuthanizationsPerMonth)/AveEuthanizationsPerMonth) %>% 
  gather(.,key = "Outcome", value = Deviation, 2:3) %>% as.data.frame() %>% 
  ggplot(aes(x = Month, y = Deviation, col = Outcome)) + geom_point() + 
  geom_line(aes(group=Outcome)) + scale_color_manual(values=c("blue","red")) + ylab("% Diff from Mean")

rm(AveAdoptionsPerMonth,AveEuthanizationsPerMonth)

#assign danger months
data <- data %>% mutate(MonthStatus = factor(ifelse(Month=="Mar"|Month=="Apr"|Month=="May"|Month=="Jun","Danger","Neutral"), levels = c("Neutral","Danger")))

val_data <- val_data %>% mutate(MonthStatus = factor(ifelse(Month=="Mar"|Month=="Apr"|Month=="May"|Month=="Jun","Danger","Neutral"), levels = c("Neutral","Danger")))

```


### Breed Effects

Determining breed effects was more challenging, as there were 1233 unique breeds listed with much of the variety coming from mixed breed combinations. As an example, 20 randomly selected breeds and the number of animals belonging to them are shown below.

```{r m}
#20 most common breeds
data %>% group_by(Breed) %>% summarize(n = n()) %>% filter(n>10) %>% sample_n(.,20)
```

It turned out there was significant variation in euthanization rates among breeds as well. The 30 most frequently and 30 least frequently euthanized breeds with at least 10 animals per breed are shown below. As shown, some breeds are euthanized far above the average rate, while some have no record of ever being euthanized.

```{r n}
cat("\nMost Frequently Euthanized Breeds\n")
#30 most commonly euthanized breeds, filtered by n >= 10
data %>% group_by(Breed) %>% summarize(n = n(), euth_prop = mean(Euthanized == TRUE)) %>% arrange(-euth_prop) %>% filter(n >= 10) %>% print(n=30)
cat("\nLeast Frequently Euthanized Breeds\n")
#30 least commonly euthanized breeds, filtered by n >= 10
data %>% group_by(Breed) %>% summarize(n = n(), euth_prop = mean(Euthanized == TRUE)) %>% arrange(euth_prop) %>% filter(n >= 10) %>% print(n=30)
```

Again in an attempt to reduce dimensionality, we set out to divide the breeds into "Danger Breeds" (those at high risk of euthanization), "Safe Breeds" (those unlikely to get euthanized), and "Neutral Breeds" (the rest in between). First we removed any "Mix" labels and separated top 30 breed strings into distinct breeds as shown in the table below. 30 breeds were selected as potential "Danger Breeds" because, as the data show, these generally represent breeds that are euthanized at least twice as frequently as average breeds. Any fraction could be chosen though, and this would be a good opportunity for future tuning.

```{r o}
#top 30 most commonly euthanized breeds, with breeds separated
data %>% mutate(Breed2 = str_remove(Breed," Mix")) %>% group_by(Breed2) %>% summarize(n = n(), euth_prop = mean(Euthanized == TRUE)) %>% arrange(-euth_prop) %>% filter(n >= 10) %>% slice_max(., order_by = euth_prop, n = 30) %>% separate(., Breed2, into = c("1","2","3"), sep = "/") %>% print(n=nrow(.))
```

We then identified the unique individual breeds that showed up anywhere in this matrix. The results are shown below in order of frequency.

```{r p}
#single breeds counted in this group
top30breeds <- data %>% mutate(Breed2 = str_remove(Breed," Mix")) %>% group_by(Breed2) %>% summarize(n = n(), euth_prop = mean(Euthanized == TRUE)) %>% arrange(-euth_prop) %>% filter(n >= 10) %>% slice_max(., order_by = euth_prop, n = 30) %>% separate(., Breed2, into = c("1","2","3"), sep = "/")
c(as.vector(top30breeds$`1`),as.vector(top30breeds$`2`),as.vector(top30breeds$`3`)) %>% table() %>% as.data.frame() %>% arrange(-Freq)
rm(top30breeds)
```

Next, we analyzed the 60 least frequently euthanized breeds. 60 was chosen because more breeds are in the safe zone than in the danger zone, and the majority in the lower 60 are never euthanized (48 to be exact). The remainder are euthanized at a rate less than half of the average animal rate, with the highest group member facing a euthanization rate of 2.7%. Again, this threshold could be set anywhere and should be the subject of optimization in future studies.

The unique breed names extracted from this "Safe Breeds" list are shown below in order of frequency counted.
```{r q}
#single breeds counted in this group
bottom60breeds <- data %>% mutate(Breed2 = str_remove(Breed," Mix")) %>% group_by(Breed2) %>% summarize(n = n(), euth_prop = mean(Euthanized == TRUE)) %>% arrange(euth_prop) %>% filter(n >= 10) %>% slice_min(., order_by = euth_prop, n = 60) %>% separate(., Breed2, into = c("1","2","3"), sep = "/")
c(as.vector(bottom60breeds$`1`),as.vector(bottom60breeds$`2`),as.vector(bottom60breeds$`3`)) %>% table() %>% as.data.frame() %>% arrange(-Freq)
rm(bottom60breeds)
```

It was apparent that some breeds, such as Labrador Retrievers, show up frequently on both lists, indicating that they are not necessarily any more or less likely to be euthanized, but rather that they are mixed with other breeds quite frequently. To account for this, we extracted only the breeds that were unique between the two lists, and used these as our final lists of "Safe" vs. "Danger" breeds. Each observation in the training and validation data sets were then analyzed to see if the Breed string contained any element of either list. The results were stored in a new feature called "Breed Status", the summary of which is shown for the training data below.

```{r r}
#define danger vs. safe breeds
bottom60breeds <- data %>% mutate(Breed2 = str_remove(Breed," Mix")) %>% group_by(Breed2) %>% summarize(n = n(), euth_prop = mean(Euthanized == TRUE)) %>% arrange(euth_prop) %>% filter(n >= 10) %>% slice_min(., order_by = euth_prop, n = 60) %>% separate(., Breed2, into = c("1","2","3"), sep = "/")
bottom60breeds2 <- c(as.vector(bottom60breeds$`1`),as.vector(bottom60breeds$`2`),as.vector(bottom60breeds$`3`)) %>% table() %>% as.data.frame()
colnames(bottom60breeds2) <- c("Breed","Freq")

top30breeds <- data %>% mutate(Breed2 = str_remove(Breed," Mix")) %>% group_by(Breed2) %>% summarize(n = n(), euth_prop = mean(Euthanized == TRUE)) %>% arrange(-euth_prop) %>% filter(n >= 10) %>% slice_max(., order_by = euth_prop, n = 30) %>% separate(., Breed2, into = c("1","2","3"), sep = "/")
top30breeds2 <- c(as.vector(top30breeds$`1`),as.vector(top30breeds$`2`),as.vector(top30breeds$`3`)) %>% table() %>% as.data.frame() %>% arrange(-Freq)
colnames(top30breeds2) <- c("Breed","Freq")
hi_topbreeds <- c(as.vector(top30breeds$`1`),as.vector(top30breeds$`2`),as.vector(top30breeds$`3`)) %>% table() %>% as.data.frame() %>% arrange(-Freq)
colnames(hi_topbreeds) <- c("Breed","Freq")

danger_breeds <- hi_topbreeds[!(hi_topbreeds$Breed %in% bottom60breeds2$Breed),1] %>% as.character()

safe_breeds <- bottom60breeds2[!(bottom60breeds2$Breed %in% top30breeds2$Breed),1] %>% as.character()

#define function for detecting any of the breeds in breed string
multi_string_detect<-function(x,y){
  temp<-sapply(y, function(z){str_detect(x, z)})
  apply(temp, 1, any)
}

#extract and assign danger/safe breeds in training data
BreedStatusTable <- data.frame(Breed = data$Breed, Danger = multi_string_detect(data$Breed,danger_breeds), Safe = multi_string_detect(data$Breed,safe_breeds)) %>% mutate(BreedStatus = ifelse(Danger == TRUE, "Danger", ifelse(Safe == TRUE, "Safe", "Neutral")))

data <- data %>% mutate(BreedStatus = factor(BreedStatusTable$BreedStatus, levels = c("Neutral", "Safe", "Danger"), ordered = FALSE))

#extract and assign danger/safe breeds in validation data
BreedStatusTable_val <- data.frame(Breed = val_data$Breed, Danger = multi_string_detect(val_data$Breed,danger_breeds), Safe = multi_string_detect(val_data$Breed,safe_breeds)) %>% mutate(BreedStatus = ifelse(Danger == TRUE, "Danger", ifelse(Safe == TRUE, "Safe", "Neutral")))

val_data <- val_data %>% mutate(BreedStatus = factor(BreedStatusTable_val$BreedStatus, levels = c("Neutral", "Safe", "Danger"), ordered = FALSE))

#clean up
rm(bottom60breeds,bottom60breeds2,BreedStatusTable,BreedStatusTable_val,hi_topbreeds,top30breeds,top30breeds2)

cat("\nCounts of Breed Statuses in Training Set\n")
table(data$BreedStatus)
```

The euthanization rates among these groups were then analyzed and tested with a Chi Squared test to confirm statistically different proportions (results shown below). Significance was observed, and so these features were carried into the model development.

```{r s}
data %>% group_by(BreedStatus) %>% summarize(euth_prop = mean(Euthanized == TRUE)) %>% arrange(-euth_prop)
chisq.test(data$BreedStatus,data$Euthanized)
```

### Color Effects

Lastly, we tried to perform a similar analysis on the animals' colors, which also came in various combinations, making up a total of 342 unique colors reported. The 30 most frequently and 30 least frequently euthanized colors are shown below, separated into unique strings and organized by frequency of occurrence within each group. 

```{r t}
#top colors
top30colors <- data %>% group_by(Color) %>% summarize(n = n(), euth_prop = mean(Euthanized == TRUE)) %>% arrange(-euth_prop) %>% filter(n >= 10) %>% slice_max(., order_by = euth_prop, n = 30) %>% separate(., Color, into = c("1","2","3","4","5","6"), sep = "/| ")
cat("\nMost Frequently Euthanized Unique Colors\n")
c(as.vector(top30colors$`1`),as.vector(top30colors$`2`),as.vector(top30colors$`3`),as.vector(top30colors$`4`)) %>% table() %>% as.data.frame() %>% arrange(-Freq)
rm(top30colors)

#bottom colors
bottom30colors <- data %>% group_by(Color) %>% summarize(n = n(), euth_prop = mean(Euthanized == TRUE)) %>% arrange(euth_prop) %>% filter(n >= 10) %>% slice_min(., order_by = euth_prop, n = 30) %>% separate(., Color, into = c("1","2","3","4","5","6"), sep = "/| ")
cat("\nLeast Frequently Euthanized Unique Colors\n")
c(as.vector(bottom30colors$`1`),as.vector(bottom30colors$`2`),as.vector(bottom30colors$`3`),as.vector(bottom30colors$`4`)) %>% table() %>% as.data.frame() %>% arrange(-Freq)
rm(bottom30colors)
```

There was clearly a lot of overlap between these groups (white, black, blue, tan, etc), so we did not use color as a predictor in the models. However, there are a few colors that do appear to be unique to the groups such as "tricolor", so perhaps additional analysis of these would be helpful in future studies.


### Final Data

The data columns that were chosen as likely predictors were then separated out and saved as final data sets for model training and evaluation. The first 10 rows of the final training set are shown below.

```{r u}
#set AnimalType as factor
data$AnimalType <- factor(data$AnimalType, levels = c("Cat","Dog"), ordered = FALSE)
val_data$AnimalType <- factor(val_data$AnimalType, levels = c("Cat","Dog"), ordered = FALSE)

#set Euthanized as factor
data$Euthanized <- factor(data$Euthanized, levels = c("FALSE","TRUE"), ordered = FALSE)
val_data$Euthanized <- factor(val_data$Euthanized, levels = c("FALSE","TRUE"), ordered = FALSE)

#produce data tables with final predictors
final_data <- data %>% select(AnimalType, AgeInYears, FixedStatus, Sex, MonthStatus, BreedStatus, Euthanized)
final_val_data <- val_data %>% select(AnimalType, AgeInYears, FixedStatus, Sex, MonthStatus, BreedStatus, Euthanized)

#print top 10 rows
head(final_data, n=10)
```


## Predictive Models

### Logistic Regression

We first fit a logistic regression model to the training data, the coefficients of which are shown below. With this model, Breed Status and Fixed Status demonstrated the strongest predictive power, though all predictors, with the exception of Month Status, were significant.

```{r v}
#fit linear model
glm_fit <- glm(data = final_data, Euthanized ~ AnimalType + AgeInYears + FixedStatus + Sex + MonthStatus + BreedStatus, family = binomial(link = "logit"))

#print coefficients
summary(glm_fit)$coefficients
```

Because euthanization is a rare event, overall accuracy was likely to be a poor measure of model performance. Instead we generated an ROC curve for each model and analyzed the area under the curve (AUC). An AUC of 1 would indicate a perfect model, that is, a model that can identify all the true positives while generating zero false positives. We did not expect to achieve this result, but our goal was to get as close as possible. The ROC curve and resulting AUC on the training data are shown for the logistic regression model below. 

```{r x}
#predict conditional probability of euthanization
glm_probs <- predict(glm_fit, newdata = final_data, type = "response")

#### ROC CURVE ####

#define TPR function
calcTPR <- function(probs, cutoff){
  #generate predictions
  predictions <- factor(probs >= cutoff, levels = c("FALSE","TRUE"))
  #produce confusion matrix
  cm <- confusionMatrix(predictions, reference = final_data$Euthanized, positive = "TRUE")
  #measure true positive rate
  TPR <- cm$table[2,2]/(cm$table[2,2] + cm$table[1,2])
  #print TPR
  TPR
}

#define FPR function
calcFPR <- function(probs, cutoff){
  #generate predictions
  predictions <- factor(probs >= cutoff, levels = c("FALSE","TRUE"))
  #produce confusion matrix
  cm <- confusionMatrix(predictions, reference = final_data$Euthanized, positive = "TRUE")
  #measure true positive rate
  FPR <- cm$table[2,1]/(cm$table[2,1] + cm$table[1,1])
  #print TPR
  FPR
}

#produce curve values
cutoffs <- seq(0,1,0.01)
ROC_glm_train <- data.frame(Cutoff = numeric(length = length(cutoffs)), TPR = numeric(length = length(cutoffs)), FPR = numeric(length = length(cutoffs)))
for(i in 1:length(cutoffs)){
  ROC_glm_train[i,1] <- cutoffs[i]
  ROC_glm_train[i,2] <- calcTPR(glm_probs,cutoffs[i])
  ROC_glm_train[i,3] <- calcFPR(glm_probs,cutoffs[i])
}

#plot curve
curve <- ROC_glm_train %>% ggplot(aes(x = FPR, y = TPR)) + 
  geom_line(col="red3",size=1.5) + geom_abline(intercept = 0, slope = 1) + 
  scale_x_continuous(limits=c(0, 1), expand = c(0, 0)) + 
  scale_y_continuous(limits=c(0, 1), expand = c(0, 0)) + 
  theme_minimal() + xlab("False Positive Rate") + ylab("True Positive Rate") +
  ggtitle("Training ROC, Logistic Regression")

print(curve)

#calculate AUC
auc_glm_train <- integrate(approxfun(x = ROC_glm_train$FPR, y = ROC_glm_train$TPR, ties = mean), min(ROC_glm_train$FPR), max(ROC_glm_train$FPR),subdivisions = 2000)$value

#print AUC
cat("\nTraining AUC, Logistic Regression:\n",auc_glm_train,"\n")

#clean up
rm(curve,cutoffs,i)
```

0.799 is not a terrible AUC, but it was certain to go down when we ran the model on the validation data, so we tried a few other models first to see if we could improve upon this value.


### K-Nearest Neighbors

Then then tried to fit a KNN model to the training data. Because euclidean distance was to be used to calculate the proximity of each observation, we first converted all predictors to numerical values. We also standardized them to all be within a range from 0 to 1. This way no predictor would be weighted more heavily than another. In cases where more than two factors existed in a feature, such as in Fixed Status and Breed Status, the distance between 0 and 1 of the middle factor was assigned based on the observed euthanization frequency in the training data, relative to the frequencies of the outer factors.

The results of the KNN model on the training data are shown below.

```{r y}
#convert features to numeric and scale
knn_data <- final_data
knn_data$AnimalType <- as.numeric(ifelse(knn_data$AnimalType == "Cat",1,0))
knn_data$AgeInYears[which(is.na(knn_data$AgeInYears)==TRUE)] <- median(na.omit(knn_data$AgeInYears))
knn_data$AgeInYears <- knn_data$AgeInYears/max(knn_data$AgeInYears)
knn_data$FixedStatus <- as.numeric(ifelse(knn_data$FixedStatus == "Spayed-Neutered",1,ifelse(knn_data$FixedStatus == "Unknown",0.3,0)))
knn_data$Sex <- as.numeric(ifelse(knn_data$Sex == "Female",1,ifelse(knn_data$FixedStatus == "Unknown",0.5,0)))
knn_data$MonthStatus <- as.numeric(ifelse(knn_data$MonthStatus == "Danger",1,0))
knn_data$BreedStatus <- as.numeric(ifelse(knn_data$BreedStatus == "Danger",1,ifelse(knn_data$BreedStatus == "Neutral",0.3,0)))
#breed codings determined by final_data %>% group_by(BreedStatus) %>% summarize(euth_prop = mean(Euthanized == TRUE))
#then 0.3 = (neut_prop - safe_prop)/(danger_prop - safe_prop)

#train model
knn_fit <- train(Euthanized ~ ., data = knn_data, method = "knn")

knn_fit
```


The tuning parameters within the KNN function resulted in an optimal k size of 9, but this was trained based on accuracy, so we again checked the ROC and AUC (shown below), and we shown a marked improvement over the logistic regression model.

```{r z}
#get probabilities
knn_probs <- predict(knn_fit, type = "prob")[,2]

#generate ROC
cutoffs <- seq(0,1,0.01)
ROC_knn_train <- data.frame(Cutoff = numeric(length = length(cutoffs)), TPR = numeric(length = length(cutoffs)), FPR = numeric(length = length(cutoffs)))
for(i in 1:length(cutoffs)){
  ROC_knn_train[i,1] <- cutoffs[i]
  ROC_knn_train[i,2] <- calcTPR(knn_probs,cutoffs[i])
  ROC_knn_train[i,3] <- calcFPR(knn_probs,cutoffs[i])
}

#plot curve
curve <- ROC_knn_train %>% ggplot(aes(x = FPR, y = TPR)) + 
  geom_line(col="red3",size=1.5) + geom_abline(intercept = 0, slope = 1) + 
  scale_x_continuous(limits=c(0, 1), expand = c(0, 0)) + 
  scale_y_continuous(limits=c(0, 1), expand = c(0, 0)) + 
  theme_minimal() + xlab("False Positive Rate") + ylab("True Positive Rate") +
  ggtitle("Training ROC, KNN")

print(curve)

#calculate AUC
auc_knn_train <- integrate(approxfun(x = ROC_knn_train$FPR, y = ROC_knn_train$TPR, ties = mean), min(ROC_knn_train$FPR), max(ROC_knn_train$FPR),subdivisions = 2000)$value

#print AUC
cat("\nTraining AUC, K Nearest Neighbors:\n",auc_knn_train,"\n")

#clean up
rm(curve,cutoffs,i,knn_data)
```


### Random Forest

The last model attempted as a random forest (decision tree) model. This model has a lot of tuning options, so we tested a few of the key tuning parameters in order to optimize our performance.

First, we tuned mtry, the number of features that may be randomly selected from at any split point. The resulting AUCs for each input between 1 and 6 are shown below. Mtry = 2 was carried forward.

```{r aa}
#create RF dataset and fill NAs
rf_data <- final_data
rf_data$AgeInYears[which(is.na(rf_data$AgeInYears)==TRUE)] <- median(na.omit(rf_data$AgeInYears))

#### TUNE ####
if(version$year >= 2019) {
  set.seed(2, sample.kind="Rounding")
} else {
  set.seed(2)
}

#define mtry tuning function
rf_mtry <- function(x){
  #train random forest
  rf_fit_fxn <- randomForest(Euthanized ~ ., data = rf_data, ntree = 300, sampsize = c(500,500), importance = TRUE, mtry = x)
  
  #get predictions
  rf_probs_fxn <- predict(rf_fit_fxn, type = "prob")[,2]

  #generate ROC
  cutoffs_fxn <- seq(0,1,0.01)

  ROC_fxn <- data.frame(Cutoff = numeric(length = length(cutoffs_fxn)), TPR = numeric(length = length(cutoffs_fxn)), FPR = numeric(length = length(cutoffs_fxn)))
  for(i in 1:length(cutoffs_fxn)){
    ROC_fxn[i,1] <- cutoffs_fxn[i]
    ROC_fxn[i,2] <- calcTPR(rf_probs_fxn,cutoffs_fxn[i])
    ROC_fxn[i,3] <- calcFPR(rf_probs_fxn,cutoffs_fxn[i])
  }

  #calculate and return AUC
  auc_fxn <- integrate(approxfun(x = ROC_fxn$FPR, y = ROC_fxn$TPR, ties = mean), min(ROC_fxn$FPR), max(ROC_fxn$FPR),subdivisions=2000)$value
  return(auc_fxn)
}

#try different values for mtry
mtrys <- seq(1:6)
data.frame(mtry = mtrys, AUC = sapply(mtrys,FUN=rf_mtry))
```
 
 
We then tuned the ntree parameter, which is the number of trees generated in the model and averaged together to produce final predictions. The resulting AUCs from tree counts ranging from 1 to 850 are shown below. 

```{r ab}
#define ntree tuning function
rf_ntree <- function(x){
  #train random forest
  rf_fit_fxn <- randomForest(Euthanized ~ ., data = rf_data, ntree = x, sampsize = c(500,500), importance = TRUE, mtry = 2)
  
  #get predictions
  rf_probs_fxn <- predict(rf_fit_fxn, type = "prob")[,2]
  
  #generate ROC
  cutoffs_fxn <- seq(0,1,0.01)
  
  ROC_fxn <- data.frame(Cutoff = numeric(length = length(cutoffs_fxn)), TPR = numeric(length = length(cutoffs_fxn)), FPR = numeric(length = length(cutoffs_fxn)))
  for(i in 1:length(cutoffs_fxn)){
    ROC_fxn[i,1] <- cutoffs_fxn[i]
    ROC_fxn[i,2] <- calcTPR(rf_probs_fxn,cutoffs_fxn[i])
    ROC_fxn[i,3] <- calcFPR(rf_probs_fxn,cutoffs_fxn[i])
  }
  
  #calculate and return AUC
  auc_fxn <- integrate(approxfun(x = ROC_fxn$FPR, y = ROC_fxn$TPR, ties = mean), min(ROC_fxn$FPR), max(ROC_fxn$FPR),subdivisions=2000)$value
  return(auc_fxn)
}

#try different values of ntree
ntrees <- c(1,3,5,10,15,25,50,75,125,200,325,525,850)
data.frame(ntree = ntrees, AUC = sapply(ntrees,FUN=rf_ntree))
```


It appeared that the AUC started to stabilize after 200 trees or so, so an additional tuning test was run in this range, the results of which are shown below.

```{r ac}
#try a more finely-tuned ntree series past 200
ntrees <- c(200,225,250,275,300,325,350,375,400)
data.frame(ntree = ntrees, AUC = sapply(ntrees,FUN=rf_ntree))
```

Ntree = 200 was carried forward into the final tuning test, which tested the sample size. Note that in each test, the samples drawn from both euthanized and non-euthanized observation groups were equal. Deviations from equality were tested as well but yielded poor results (not shown). The results of sample size tuning on AUC values are shown below.

```{r ad}
#define sample size tuning function
rf_sampsize <- function(x){
  #train random forest
  rf_fit_fxn <- randomForest(Euthanized ~ ., data = rf_data, ntree = 200, sampsize = c(x,x), importance = TRUE, mtry = 2)
  
  #get predictions
  rf_probs_fxn <- predict(rf_fit_fxn, type = "prob")[,2]
  
  #generate ROC
  cutoffs_fxn <- seq(0,1,0.01)
  
  ROC_fxn <- data.frame(Cutoff = numeric(length = length(cutoffs_fxn)), TPR = numeric(length = length(cutoffs_fxn)), FPR = numeric(length = length(cutoffs_fxn)))
  for(i in 1:length(cutoffs_fxn)){
    ROC_fxn[i,1] <- cutoffs_fxn[i]
    ROC_fxn[i,2] <- calcTPR(rf_probs_fxn,cutoffs_fxn[i])
    ROC_fxn[i,3] <- calcFPR(rf_probs_fxn,cutoffs_fxn[i])
  }
  
  #calculate and return AUC
  auc_fxn <- integrate(approxfun(x = ROC_fxn$FPR, y = ROC_fxn$TPR, ties = mean), min(ROC_fxn$FPR), max(ROC_fxn$FPR),subdivisions = 2000)$value
  return(auc_fxn)
}

#try different values for sampsize
sampsizes <- c(25,50,75,100,200,350,500,750,1000)
data.frame(sampsize = sampsizes, AUC = sapply(sampsizes,FUN=rf_sampsize))
```

A sample size of 100 was found to perform best, and so the final model was fit on the training data with these parameters, and the resulting ROC curve and AUC (shown below) were recorded.

```{r ae}
#clean up
rm(rf_data,ntrees,mtrys,sampsizes)

#create RF dataset and fill NAs
rf_data <- final_data
rf_data$AgeInYears[which(is.na(rf_data$AgeInYears)==TRUE)] <- median(na.omit(rf_data$AgeInYears))

#### FINAL MODEL ####

#train random forest
rf_fit <- randomForest(Euthanized ~ ., data = rf_data, ntree = 200, sampsize = c(100,100), importance = TRUE, mtry = 2)

#get predictions
rf_probs <- predict(rf_fit, type = "prob")[,2]

#generate ROC
cutoffs <- seq(0,1,0.01)
ROC_rf_train <- data.frame(Cutoff = numeric(length = length(cutoffs)), TPR = numeric(length = length(cutoffs)), FPR = numeric(length = length(cutoffs)))
for(i in 1:length(cutoffs)){
  ROC_rf_train[i,1] <- cutoffs[i]
  ROC_rf_train[i,2] <- calcTPR(rf_probs,cutoffs[i])
  ROC_rf_train[i,3] <- calcFPR(rf_probs,cutoffs[i])
}

#plot curve
curve <- ROC_rf_train %>% ggplot(aes(x = FPR, y = TPR)) + 
  geom_line(col="red3",size=1.5) + geom_abline(intercept = 0, slope = 1) + 
  scale_x_continuous(limits=c(0, 1), expand = c(0, 0)) + 
  scale_y_continuous(limits=c(0, 1), expand = c(0, 0)) + 
  theme_minimal() + xlab("False Positive Rate") + ylab("True Positive Rate") +
  ggtitle("Training ROC, Random Forest")

print(curve)

#calculate AUC
auc_rf_train <- integrate(approxfun(x = ROC_rf_train$FPR, y = ROC_rf_train$TPR, ties = mean), min(ROC_rf_train$FPR), max(ROC_rf_train$FPR),subdivisions = 2000)$value

#print AUC
cat("\nTraining AUC, Random Forest:\n",auc_rf_train,"\n")

#clean up
rm(curve,cutoffs,i,rf_data)
```

Variable importance data from the final random forest model was captured and is presented below. The most impactful predictors in this model seemed to be Fixed Status and Age, followed by Breed Status. It is notable that Age was not as impactful in the logistic regression model, indicating that perhaps this feature exhibits a highly non-linear relationship with outcome.

```{r zz}
cat("\nImportance of Variables in Random Forest Model:\n")
rf_fit$importance
```

### Model Comparison

The three models were then compared, as shown below. The best performing model, at least on the training data, was KNN, followed by random forest, followed by logistic regression.

```{r af}
#combine all training ROC data
all_ROCs_train <- rbind(mutate(ROC_glm_train,Method = rep("GLM",101)), 
                        mutate(ROC_knn_train,Method = rep("KNN",101)), 
                        mutate(ROC_rf_train, Method = rep("RandomForest",101)))

#plot all training ROCs together
allplot <- all_ROCs_train %>% ggplot(aes(x = FPR, y = TPR, col = Method)) + 
  geom_line(size=1) + geom_abline(intercept = 0, slope = 1, col = "gray") + 
  scale_x_continuous(limits=c(0, 1), expand = c(0, 0)) + 
  scale_y_continuous(limits=c(0, 1), expand = c(0, 0)) + 
  theme_minimal() + xlab("False Positive Rate") + 
  ylab("True Positive Rate") + ggtitle("Training ROC Curves")

print(allplot)
rm(allplot)

#print all AUCs
data.frame(Model = c("Logistic Regression","K Nearest Neighbors","Random Forest"), Training_AUC = c(auc_glm_train,auc_knn_train,auc_rf_train)) %>% arrange(-Training_AUC)

```


### Model Validation

We then tested our three models on the validation data set. As expected, the resulting AUCs were lower than on the training data, but still not terrible. In terms of comparison, the random forest model actually performed best on the validation data, retaining a similar AUC from training into validation (0.817 to 0.803). The KNN performance was reduced the most, making it the worst performing model on the validation set.

```{r aff}
#new logistic regression predictions
glm_probs_val <- predict(glm_fit, newdata = final_val_data, type = "response")

#new knn predictions
#first recreate knn data set
knn_data <- final_data
knn_data$AnimalType <- as.numeric(ifelse(knn_data$AnimalType == "Cat",1,0))
knn_data$AgeInYears[which(is.na(knn_data$AgeInYears)==TRUE)] <- median(na.omit(knn_data$AgeInYears))
knn_data$AgeInYears <- knn_data$AgeInYears/max(knn_data$AgeInYears)
knn_data$FixedStatus <- as.numeric(ifelse(knn_data$FixedStatus == "Spayed-Neutered",1,ifelse(knn_data$FixedStatus == "Unknown",0.3,0)))
knn_data$Sex <- as.numeric(ifelse(knn_data$Sex == "Female",1,ifelse(knn_data$FixedStatus == "Unknown",0.5,0)))
knn_data$MonthStatus <- as.numeric(ifelse(knn_data$MonthStatus == "Danger",1,0))
knn_data$BreedStatus <- as.numeric(ifelse(knn_data$BreedStatus == "Danger",1,ifelse(knn_data$BreedStatus == "Neutral",0.3,0)))

#then treat validation data similarly
knn_val_data <- final_val_data
knn_val_data$AnimalType <- as.numeric(ifelse(knn_val_data$AnimalType == "Cat",1,0))
knn_val_data$AgeInYears[which(is.na(knn_val_data$AgeInYears)==TRUE)] <- median(na.omit(knn_data$AgeInYears))
knn_val_data$AgeInYears <- knn_val_data$AgeInYears/max(knn_data$AgeInYears)
knn_val_data$FixedStatus <- as.numeric(ifelse(knn_val_data$FixedStatus == "Spayed-Neutered",1,ifelse(knn_val_data$FixedStatus == "Unknown",0.3,0)))
knn_val_data$Sex <- as.numeric(ifelse(knn_val_data$Sex == "Female",1,ifelse(knn_val_data$FixedStatus == "Unknown",0.5,0)))
knn_val_data$MonthStatus <- as.numeric(ifelse(knn_val_data$MonthStatus == "Danger",1,0))
knn_val_data$BreedStatus <- as.numeric(ifelse(knn_val_data$BreedStatus == "Danger",1,ifelse(knn_val_data$BreedStatus == "Neutral",0.3,0)))

#make predictions
knn_probs_val <- predict(knn_fit, newdata = knn_val_data, type = "prob")[,2]

#clean up
rm(knn_data,knn_val_data)

#new random forest predictions
rf_probs_val <- predict(rf_fit, newdata = final_val_data, type = "prob")[,2]

#### ROC AND AUC CALCULATIONS ####

#define TPR function for validation set
calcTPR_val <- function(probs, cutoff){
  #generate predictions
  predictions <- factor(probs >= cutoff, levels = c("FALSE","TRUE"))
  #produce confusion matrix
  cm <- confusionMatrix(predictions, reference = final_val_data$Euthanized, positive = "TRUE")
  #measure true positive rate
  TPR <- cm$table[2,2]/(cm$table[2,2] + cm$table[1,2])
  #print TPR
  TPR
}

#define FPR function for validation set
calcFPR_val <- function(probs, cutoff){
  #generate predictions
  predictions <- factor(probs >= cutoff, levels = c("FALSE","TRUE"))
  #produce confusion matrix
  cm <- confusionMatrix(predictions, reference = final_val_data$Euthanized, positive = "TRUE")
  #measure true positive rate
  FPR <- cm$table[2,1]/(cm$table[2,1] + cm$table[1,1])
  #print TPR
  FPR
}

#define cutoffs
cutoffs <- seq(0,1,0.01)

#logit ROC and AUC on validation set
ROC_glm_val <- data.frame(Cutoff = numeric(length = length(cutoffs)), TPR = numeric(length = length(cutoffs)), FPR = numeric(length = length(cutoffs)))
for(i in 1:length(cutoffs)){
  ROC_glm_val[i,1] <- cutoffs[i]
  ROC_glm_val[i,2] <- calcTPR_val(glm_probs_val,cutoffs[i])
  ROC_glm_val[i,3] <- calcFPR_val(glm_probs_val,cutoffs[i])
}

auc_glm_val <- integrate(approxfun(x = ROC_glm_val$FPR, y = ROC_glm_val$TPR, ties = mean), min(ROC_glm_val$FPR), max(ROC_glm_val$FPR),subdivisions = 2000)$value

#knn ROC and AUC on validation set
ROC_knn_val <- data.frame(Cutoff = numeric(length = length(cutoffs)), TPR = numeric(length = length(cutoffs)), FPR = numeric(length = length(cutoffs)))
for(i in 1:length(cutoffs)){
  ROC_knn_val[i,1] <- cutoffs[i]
  ROC_knn_val[i,2] <- calcTPR_val(knn_probs_val,cutoffs[i])
  ROC_knn_val[i,3] <- calcFPR_val(knn_probs_val,cutoffs[i])
}

auc_knn_val <- integrate(approxfun(x = ROC_knn_val$FPR, y = ROC_knn_val$TPR, ties = mean), min(ROC_knn_val$FPR), max(ROC_knn_val$FPR),subdivisions = 2000)$value

#random forest ROC and AUC on validation set
ROC_rf_val <- data.frame(Cutoff = numeric(length = length(cutoffs)), TPR = numeric(length = length(cutoffs)), FPR = numeric(length = length(cutoffs)))
for(i in 1:length(cutoffs)){
  ROC_rf_val[i,1] <- cutoffs[i]
  ROC_rf_val[i,2] <- calcTPR_val(rf_probs_val,cutoffs[i])
  ROC_rf_val[i,3] <- calcFPR_val(rf_probs_val,cutoffs[i])
}

auc_rf_val <- integrate(approxfun(x = ROC_rf_val$FPR, y = ROC_rf_val$TPR, ties = mean), min(ROC_rf_val$FPR), max(ROC_rf_val$FPR),subdivisions = 2000)$value

#combine all training ROC data
all_ROCs_val <- rbind(mutate(ROC_glm_val,Method = rep("GLM",101)), 
                        mutate(ROC_knn_val,Method = rep("KNN",101)), 
                        mutate(ROC_rf_val, Method = rep("RandomForest",101)))

#plot all training ROCs together
allplot <- all_ROCs_val %>% ggplot(aes(x = FPR, y = TPR, col = Method)) + 
  geom_line(size=1) + geom_abline(intercept = 0, slope = 1, col = "gray") + 
  scale_x_continuous(limits=c(0, 1), expand = c(0, 0)) + 
  scale_y_continuous(limits=c(0, 1), expand = c(0, 0)) + 
  theme_minimal() + xlab("False Positive Rate") + 
  ylab("True Positive Rate") + ggtitle("Validation ROC Curves")

print(allplot)
rm(allplot)

#print all AUCs
data.frame(Model = c("Logistic Regression","K Nearest Neighbors","Random Forest"), Validation_AUC = c(auc_glm_val,auc_knn_val,auc_rf_val)) %>% arrange(-Validation_AUC)
```


## Conclusions

In the end, we were able to use a random forest model to identify animals that are at highest risk of euthanization with reasonable accuracy. The real-world implications of the final ROC curve would be how much false positive tolerance the clinic has, and this would most likely come down to budget. For example, if they are able to tolerate 25% false positives, meaning they'd be devoting that much budget to supplying extra attention to animals who were likely to be adopted anyway, then they would be able to correctly identify approximately 65% of the animals that are accurately at risk, and they'll ideally be able to prevent some of their deaths.

Assuming 25% false positive rate is too high (after all, they see thousands of animals per year), there are several improvements that could be made here. First, there were clear interaction effects that were not explored. We eliminated some of them by separating out the sex and fixed statuses. It's possible that particular color and breed combinations may be predictive as well, or breeds and age. Secondly, the categories and thresholds around which the months and breeds were aggregated have a lot of flexibility, and no model tuning was done while varying these. Thirdly, better validation techniques could be performed on the training data such as k-fold cross-validation or leave-one-out cross-validation. Finally, other classification models could be explored, such as support vector machines or neural networks. Overall, the models presented here serve as a great starting point to help the clinic start identifying at risk animals, and through some of the methods described here, along with additional data collection, they can only improve.