---
title: "edX-Harvard Data Science Capstone Project: Shelter Animal Outcomes"
author: "Eric Tucker"
date: "March 1st, 2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

## Introduction

Recommendation systems are growing in popularity as personalized consumer experiences are becoming common, even expected, across industries from retail to media, even food & beverage and education. The general idea is to use available information about a user's past behavior, the behavior of other users, and the items of interest to recommend a set of new items that the user may find interesting.

In this project, a recommendation algorithm for movies was built using the 10M MovieLens dataset, which contains 10,000,054 movie ratings on a scale of 1-5 stars for 69,878 users rating 10,677 movies. The data set not only contains the user/movie ID pairs and the rating, but also the timestamp of the rating, the title of the movie, and the genres in which the movie is categorized.

Models were built, or expanded, based on various components of the data, and several common machine learning methods were attempted, using root mean squared-error (RMSE) to evaluate their effectiveness. (RMSE can be thought of as the average number of "stars" away from the actual values our predictions were.) The goal of each of these attempts was to drive the RMSE on a test data set -- a set of ratings that the trained model had previously not observed -- as close to zero as possible.

The methods for data setup, exploration, and analysis are explained here, and the RMSEs generated from the resulting models are reproduced.

## Data Cleaning & Exploration

### Setup

Before manipulating any data, the libraries necessary for this project were first loaded, which included tidyverse, caret, data.table, lubridate, gridExtra, glmnet, and recosystem.

```{r libraries, include = FALSE}
package.check <- function(package.names){
  for(i in 1:length(package.names)){
    if(package.names[i] %in% installed.packages()){
      library(package.names[i], character.only = TRUE)
      cat(package.names[i],"already installed. Library reloaded.\n")
    }
    else{
      install.packages(package.names[i], character.only = TRUE, repos="http://cran.us.r-project.org")
      library(package.names[i], character.only = TRUE)
      cat("Missing package",package.names[i],"has been installed.\n")
    }
  }
  cat("All requested libraries have been loaded.\n")
}

package.check(c("tidyverse","caret","data.table","lubridate","gridExtra","glmnet","recosystem","dplyr"))
```

The data was then downloaded and split into an "edx" set, containing 90% of the data for model creation, and a "validation" set, containing 10% of the data for final RMSE evaluation. The edx data set was then further split into a "test" and "training" set to help train the models before conducting training and validation against the full data set. These data sets are summarized in the tables below: 

```{r data setup, echo = FALSE}
#download data file
dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

#pull ratings records
ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
           col.names = c("userId", "movieId", "rating", "timestamp"))

#pull movie info
movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

#create data frame of movie info
if(version$major >= 4) {
  movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
            title = as.character(title), genres = as.character(genres))
} else {
  movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
            title = as.character(title), genres = as.character(genres))
}

#join movie and ratings tables
movielens <- left_join(ratings, movies, by = "movieId")
  
#create validation set as 10% of MovieLens data
if(version$year >= 2019) {
  set.seed(1, sample.kind="Rounding")
} else {
  set.seed(1)
}
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

#make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

#add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

#clean up environment
rm(dl, ratings, movies, test_index, temp, movielens, removed)

#partition edx into test and training sets
if(version$year >= 2019) {
  set.seed(123, sample.kind="Rounding")
} else {
  set.seed(123)
}
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.1, list = FALSE)
train <- edx[-test_index,]
temp <- edx[test_index,]

#make sure userId and movieId in test set are also in training set
test <- temp %>% 
  semi_join(train, by = "movieId") %>%
  semi_join(train, by = "userId")

#add rows removed from test set back into train set
removed <- anti_join(temp, test)
train <- rbind(train, removed)

#clean up environment
rm(removed,temp,test_index)

#present data summaries
cat("Validation data set:\n")
summary(validation)
cat("edX data set:\n")
summary(edx)
cat("Training data set (subset of edX):\n")
summary(train)
cat("Test data set (subset of edX):\n")
summary(test)
```

The RMSE function that would be used for evaluation was also defined up front using the following code:

```{r define RMSE}
RMSE <- function(predictions, reference){
  sqrt(mean((reference - predictions)^2))
}
```

### Baseline Model

The simplest model, other than randomly guessing a rating, was to pick a constant rating value and use it for every prediction. The constant value that will minimize RMSE is the average rating across all data points, as this value will create the minimum cumulative distance between it and each point in the set. The average was calculated and run as a model through the RMSE function, and the outputs were assigned to the first row of a results table that was used throughout the model building process. The resulting table is shown below:

```{r average only model, echo = FALSE}
#calculate the average
overall_ave <- mean(edx$rating)

#evaluate it as a model
cat("RMSE =",RMSE(overall_ave,test$rating),"\n")

#assign result to results table
options(pillar.sigfig=6)
rmse_results <- tibble(Method = "Average Only", RMSE = RMSE(overall_ave,test$rating))
rmse_results
```

As shown, the "average only" method produces an RMSE of 1.06 on the test data. The goal of the rest of the analysis was to drive the RMSE as far below this starting point as possible.

### Movie and User Effects

Based on experience, it could be assumed movies in the set were more well-liked than others, and likely that some users were more critical of movies in general than others. The existence of this variability was confirmed by inspecting the distribution of the average rating each movie received as well as the distribution of each users' average rating given, both shown below:

```{r mean rating hists, echo = FALSE, fig.height = 3}
user_averages <- edx %>% group_by(userId) %>% summarize(user_mean_rating = mean(rating)) %>% ggplot(aes(x=user_mean_rating)) + geom_histogram(fill="darkslategrey",col="black")

movie_averages <- edx %>% group_by(movieId) %>% summarize(movie_mean_rating = mean(rating)) %>% ggplot(aes(x=movie_mean_rating)) + geom_histogram(fill="darkslategrey",col="black")

grid.arrange(user_averages,movie_averages,ncol=2)
rm(user_averages,movie_averages)
```

The histograms here demonstrate that there is indeed significant variability between movies and between users. This information was used to improve on the "average only" prediction algorithm by incorporating a movie effect, that is the amount the rated movie tended to be rated by users relative to the average, as well as a user effect, that is the amount the user tended to rate movies in general relative to the average. First, these effects were calculated independently and the impact of incorporating the effects into the predictions was tested. Resulting RMSEs are shown below:

```{r user and movie effects, echo = FALSE}
#calculate effects
user_effects <- train %>% group_by(userId) %>%
  summarize(user_effect = mean(rating - overall_ave))

movie_effects <- train %>% group_by(movieId) %>%
  summarize(movie_effect = mean(rating - overall_ave))

#check effects on RMSE:
#user effect only
temp <- left_join(test,user_effects,by = "userId")
preds <- overall_ave + temp$user_effect
cat("RMSE =",RMSE(preds,test$rating),"when user effects alone are added to the average.\n")

#movie effect only
temp <- left_join(test,movie_effects,by = "movieId")
preds <- overall_ave + temp$movie_effect
cat("RMSE =",RMSE(preds,test$rating),"when movie effects alone are added to the average.\n")

#combined
temp <- test %>% left_join(movie_effects, by = "movieId") %>% left_join(user_effects, by = "userId")
preds <- overall_ave + temp$movie_effect + temp$user_effect
cat("RMSE =",RMSE(preds,test$rating),"when movie effects and user effects are both added to the average.\n")
```

As shown, the user effect had a stronger impact on RMSE than the movie effect alone, but combined, the RMSE was reduced even further. All these findings were significant and so were added to the results table, the updated version of which is shown below:

```{r results update 1, echo = FALSE}
rmse_results <- rbind(rmse_results,tibble(Method = "Ave + User Effects",RMSE = RMSE(overall_ave + temp$user_effect,test$rating)))
rmse_results <- rbind(rmse_results,tibble(Method = "Ave + Movie Effects",RMSE = RMSE(overall_ave + temp$movie_effect,test$rating)))
rmse_results <- rbind(rmse_results,tibble(Method = "Ave + User FX + Movie FX",RMSE = RMSE(overall_ave + temp$user_effect + temp$movie_effect,test$rating)))
rmse_results
```

### Effect Adjustments

Adding effects is only maximally useful when those effects are uncorrelated, and it was suspected that a correlation might exist between user and movie effects, for example, if a particularly negative reviewer just watches movies that are particularly poorly rated, or any combination of situations like this. To test for this, the correlation was measured directly as shown below:

```{r correlation test}
cor.test(temp$user_effect,temp$movie_effect)
```

And it was found that a small but significant correlation did exist. To try and drive this down, each effect was re-evaluated after removing the other effect using the following formula:

  Effect = Rating - Overall Average - Other Effect 

And the RMSE was re-tested in each case. First the movie effects were redefined to be independent of the user effects. The resulting RMSE and remaining correlation are shown below:

```{r adjusted movie effects, echo = FALSE}
adj_movie_effects <- train %>% left_join(user_effects, by='userId') %>%
  group_by(movieId) %>% summarize(movie_effect = mean(rating - overall_ave - user_effect))
temp <- test %>% left_join(adj_movie_effects, by = "movieId") %>% left_join(user_effects, by = "userId")
preds <- overall_ave + temp$movie_effect + temp$user_effect
cat("RMSE =",RMSE(preds,test$rating),"\n")
cor.test(temp$user_effect,temp$movie_effect)
```

Indeed both the RMSE and the correlation coefficient were reduced. These results were added to our results table:

```{r results update 2, echo = FALSE}
rmse_results <- rbind(rmse_results,tibble(Method = "Ave + UserFX + Adj_MovieFX",RMSE = RMSE(preds,test$rating)))
rmse_results
```

Then the user effects were redefined to be independent of the movie effects:

```{r adjusted user effects, echo = FALSE}
adj_user_effects <- train %>% left_join(movie_effects, by='movieId') %>%
  group_by(userId) %>% summarize(user_effect = mean(rating - overall_ave - movie_effect))
temp <- test %>% left_join(adj_user_effects, by = "userId") %>% left_join(movie_effects, by = "movieId")
preds <- overall_ave + temp$movie_effect + temp$user_effect
cat("RMSE =",RMSE(preds,test$rating),"\n")
cor.test(temp$user_effect,temp$movie_effect)
```

This had a much stronger impact, reducing RMSE significantly and dropping the correlation down to a tenth of what it was before, though still significant. These results were added to the results table:

```{r results update 3, echo = FALSE}
rmse_results <- rbind(rmse_results,tibble(Method = "Ave + MovieFX + Adj_UserFX",RMSE = RMSE(preds,test$rating)))
rmse_results
```

The increased significance of the user effect adjustment over the movie effect adjustment observed here was likely related to the fact that user effects appear to have a bigger overall impact on the predicted rating than movie effects, so performing the adjustment on the user effect, even if equal in relation to the predictor itself, would impact the predictions more strongly.

Although not expected to be helpful, both adjusted effects were then combined to see what would happen:

```{r both effects adjusted, echo = FALSE}
temp <- test %>% left_join(adj_user_effects, by = "userId") %>% left_join(adj_movie_effects, by = "movieId")
preds <- overall_ave + temp$movie_effect + temp$user_effect
cat("RMSE =",RMSE(preds,test$rating),"\n")
cor.test(temp$user_effect,temp$movie_effect)
```

This increased both the RMSE and the correlation coefficient. This was also tried on the training data set (not shown), and the result was the same. The adjusted values cannot be calculated without taking the other predictor into consideration, so by combining both sets of adjusted values, some recursive operations were probably incorporated that were erasing some the gains from the individual adjustments. Ultimately, the raw movie effects combined with the adjusted user effects led to the best result, so this was carried forward, and the combined adjusted effects were not added to the results table.

### Rating Count Effects and Penalizations

It was then considered whether the number of ratings a user has submitted would affect that user's overall average rating. Plotting each user's average rating (centered at zero) against their number of ratings reveals that it actually does:

```{r plot user rating count vs ave rating, echo = FALSE, fig.height = 3}
edx %>% group_by(userId) %>% summarize(user_effect = mean(rating-overall_ave), ratings_count = n()) %>% ggplot(aes(x=ratings_count,y=user_effect)) + geom_point(alpha = 0.2) + geom_smooth()
```

As shown, the user effect tends to dip below average as the number of ratings increases. If this could improve the model, it would be easiest to add it in a linear fashion, so log transforming the data helped create a slightly more linear relationship:

```{r plot log user rating count vs ave rating, echo = FALSE, fig.height = 3}
edx %>% group_by(userId) %>% summarize(user_effect = mean(rating-overall_ave), log_ratings_count = log(n())) %>% ggplot(aes(x=log_ratings_count,y=user_effect)) + geom_point(alpha = 0.2) + geom_smooth()
```

It was likely, however, that these effects were already taken into account by the user effect already calculated. After all, this relationship attempts to predict user effect based on the number of ratings on record for that user, but the outcomes of those ratings are already known, so the effect could be calculated directly rather than estimating it. To check for any residual effects, the calculated movie effects and (adjusted) user effects were removed, and the visualization was reconstructed:

```{r plot residual user effects against rating count, echo = FALSE, fig.height=3}
edx %>% left_join(adj_user_effects, by = "userId") %>% left_join(movie_effects, by = "movieId") %>% group_by(userId) %>% summarize(resid_user_effect = mean(rating-overall_ave-user_effect-movie_effect), log_ratings_count = log(n())) %>% ggplot(aes(x=log_ratings_count,y=resid_user_effect)) + geom_point(alpha = 0.2) + geom_smooth()
```

And it became clear that no remaining measurable relationship existed.

Conducting the same analysis on movies, however, indicated that there might actually be some residual movie effect based on ratings count remaining after the movie and user effects were taken out:

```{r plot residual movie effects against rating count, echo = FALSE, fig.height=3}
edx %>% left_join(adj_user_effects, by = "userId") %>% left_join(movie_effects, by = "movieId") %>% group_by(movieId) %>% summarize(resid_movie_effect = mean(rating-overall_ave-movie_effect-user_effect), log_ratings_count = log(n())) %>% ggplot(aes(x=log_ratings_count,y=resid_movie_effect)) + geom_point(alpha = 0.2) + geom_smooth()
```

A model was fit to describe this relationship using the training data set. The details of the resulting model are shown below:

```{r movie_ct_effect fit, echo = FALSE}
movie_counts <- train %>% left_join(adj_user_effects, by = "userId") %>% left_join(movie_effects, by = "movieId") %>% group_by(movieId) %>% 
  summarize(resid_movie_effect = mean(rating-overall_ave-movie_effect-user_effect), log_ratings_count = log(n()))
fit_count <- lm(resid_movie_effect~log_ratings_count, data = movie_counts)
summary(fit_count)
```

Significant coefficients were found, so this model was added to the initial predictions, and the resulting RMSE was calculated:

```{r movie_ct_effect predictions, echo = FALSE}
#add predictions to temp file
rm(temp)
temp <- test %>% left_join(adj_user_effects, by = "userId") %>% left_join(movie_effects, by = "movieId") %>% left_join(movie_counts, by = "movieId") %>% select(-resid_movie_effect) %>% mutate(movie_ct_effect = predict(fit_count,.))

#calculate new RMSE based on all effects plus new movie count effect
preds <- overall_ave + temp$movie_effect + temp$user_effect + temp$movie_ct_effect
cat("RMSE =",RMSE(preds,test$rating),"\n")
```

This led to a slight, but nonetheless helpful reduction in the RMSE on the test set. This value was added to the results table:

```{r results update 4, echo = FALSE}
rmse_results <- rbind(rmse_results,tibble(Method = "Ave + MovieFX + Adj_UserFX + Movie_Ct_FX",RMSE = RMSE(preds,test$rating)))
rmse_results
```

The most obvious takeaway from these record count analyses, however, was that variability was significantly higher when counts were lower. Not only is the variability between the averages of users and movies higher, as shown in the previous graphs, but the variability among ratings given by a single user, or given for a single movie, as summarized by the variance, tends to be higher for users and movies with low rating counts. This is shown in the plots of variance vs. rating count below:

```{r variance vs. user rating counts, fig.height = 3, echo = FALSE}
edx %>% group_by(userId) %>% summarize(user_rating_count = n(), rating_variance = var(rating)) %>% ggplot(aes(x=user_rating_count,y=rating_variance)) + geom_point(alpha=0.2)
```

```{r variance vs. movie rating counts, fig.height = 3, echo = FALSE}
edx %>% group_by(movieId) %>% summarize(movie_rating_count = n(), rating_variance = var(rating)) %>% ggplot(aes(x=movie_rating_count,y=rating_variance)) + geom_point(alpha=0.2)
```

This suggested that less faith could be placed in the user and movie effects for users or movies with few recorded ratings. The risk that this variability imposed was attempted to be reduced by using penalty terms to drive effects from these groups closer to zero, which is the equivalent of driving the resulting predictions closer to the average. A penalty term (lambda) was added to the denominator when calculating the mean, so calculations involving very large counts would be relatively unaffected, while calculations involving small counts would be reduced more significantly by a large lambda.

This was attempted using a wide range of lambdas, and an improvement on RMSE was indeed observed. The application of penalties on movie-adjusted user effects within the most effective range of lambdas is shown below. A zero value was included to confirm the RMSE is equivalent to the previous (unpenalized) version of the model:

```{r penalized user effects, echo = FALSE, fig.height = 4}
#first reassign temp to a more long-term object 
meaningful_effects <- temp %>% select(-log_ratings_count)

#set lambdas
lambdas <- seq(from=0, to=12)

#then define penalization function
pen_uFX <- function(lambda){
  pen_adj_user_effects <- train %>% left_join(movie_effects, by = "movieId") %>% group_by(userId) %>% summarize(pen_user_effect = sum(rating - overall_ave-movie_effect)/(n()+lambda))
  temp <- meaningful_effects %>% left_join(pen_adj_user_effects,by = "userId")
  preds <- overall_ave + temp$movie_effect + temp$movie_ct_effect + temp$pen_user_effect
  rmse <- RMSE(preds,test$rating)
  rmse
}

#run function using all lambdas, and plot resulting RMSEs
rmses <- sapply(lambdas, FUN = pen_uFX)
plot(y = rmses, x = lambdas, ylab = "RMSE", xlab = "lambda")
```

As shown, the RMSE was maximally reduced through user effect penalization with a lambda value of 5. RMSE was actually reduced for all values between 1 and 9, but anything larger than 9 caused RMSE to increase. These effects were added to the model and the resulting RMSE to the results table:

```{r results update 5, echo = FALSE}
best_lambda_userFX <- lambdas[which.min(rmses)]

pen_adj_user_effects <- train %>% left_join(movie_effects, by = "movieId") %>% group_by(userId) %>% summarize(pen_user_effect = sum(rating - overall_ave-movie_effect)/(n()+best_lambda_userFX))

#add effects to test data
meaningful_effects <- meaningful_effects %>% left_join(pen_adj_user_effects,by = "userId")

#add resulting RMSE to results table
rmse_results <- rbind(rmse_results,tibble(Method = "Ave + MovieFX + Pen_Adj_UserFX + Movie_Ct_FX",RMSE = RMSE(overall_ave + meaningful_effects$movie_effect + meaningful_effects$pen_user_effect + meaningful_effects$movie_ct_effect,test$rating)))
rmse_results
```

This was attempted for the movie effects as well, and an extremely slight drop in test RMSE was noticed at a lambda of 1. Results shown below:

```{r penalized movie effects, echo = FALSE, fig.height = 4}
#set lambdas
lambdas <- seq(from=0, to=3, by=0.25)

#then define penalization function
pen_mFX <- function(lambda){
    pen_movie_effects <- train %>% group_by(movieId) %>% summarize(pen_movie_effect = sum(rating - overall_ave)/(n()+lambda))
    temp <- meaningful_effects %>% left_join(pen_movie_effects,by = "movieId")
    preds <- overall_ave + temp$pen_user_effect + temp$movie_ct_effect + temp$pen_movie_effect
    rmse <- RMSE(preds,test$rating)
    rmse
}

#run function using all lambdas, and plot resulting RMSEs
rmses <- sapply(lambdas, FUN = pen_mFX)
plot(y = rmses, x = lambdas, ylab = "RMSE", xlab = "lambda")
```

Considering that each predictor in this model is affected by the previous one, it seemed, looking back, that this penalization should have been done when first calculating the movie effects, then the penalized user effects would be calculated off those, and then the residual movie count effects would be fit based off both of those. These features were not discovered until later in the process though, so an unusual order was used. It is likely that the movie count effect and the penalized movie effects are correlated, since they are both impacted by movie rating count. For fear of overfitting, and since the test RMSE impact was so small, the penalized movie effects were left out of this initial model.

### Linear Model Re-Fit

To see how things would have turned out if analyzed in the "correct order", the first three predictors were recalculated in the order of penalized movie effects first, penalized movie-adjusted user effects second, and residual movie count effects last. The results of calculating the first two are shown below:

```{r re-run of meaningful predictors, echo = FALSE, fig.height = 4}
#penalized movie effects first
#set lambdas
lambdas <- seq(from=0, to=3, by=0.25)

#define penalization function
pen_mFX2 <- function(lambda){
    pen_movie_effects2 <- train %>% group_by(movieId) %>% summarize(pen_movie_effect = sum(rating - overall_ave)/(n()+lambda))
    temp <- test %>% left_join(pen_movie_effects2,by = "movieId")
    preds <- overall_ave + temp$pen_movie_effect
    rmse <- RMSE(preds,test$rating)
    rmse
}

#run function using all lambdas, and plot resulting RMSEs
rmses <- sapply(lambdas, FUN = pen_mFX2)
plot(y = rmses, x = lambdas, ylab = "RMSE", xlab = "lambda", main = "Movie Effect Penalization")

#find best lambda for penalized movie effects
best_lambda_movieFX2 <- lambdas[which.min(rmses)]

pen_movie_effects2 <- train %>% group_by(movieId) %>% summarize(pen_movie_effect = sum(rating - overall_ave)/(n()+best_lambda_movieFX2))

#add effects to test data
meaningful_effects2 <- test %>% left_join(pen_movie_effects2, by = "movieId")

#confirm RMSE
cat("RMSE =", RMSE(overall_ave + meaningful_effects2$pen_movie_effect,test$rating), "for pen. movie effect only.\n")

#then calculate penalized, movie-adjusted user effects
#set lambdas
lambdas <- seq(from=0, to=10, by=0.5)

#define penalization function
pen_uFX2 <- function(lambda){
  pen_adj_user_effects2 <- train %>% left_join(pen_movie_effects2, by = "movieId") %>% group_by(userId) %>% summarize(pen_user_effect = sum(rating - overall_ave - pen_movie_effect)/(n()+lambda))
  temp <- meaningful_effects2 %>% left_join(pen_adj_user_effects2,by = "userId")
  preds <- overall_ave + temp$pen_movie_effect + temp$pen_user_effect
  rmse <- RMSE(preds,test$rating)
  rmse
}

#run function using all lambdas, and plot resulting RMSEs
rmses <- sapply(lambdas, FUN = pen_uFX2)
plot(y = rmses, x = lambdas, ylab = "RMSE", xlab = "lambda", main = "User Effect Penalization")

#find best lambda for penalized movie-adjusted user effects
best_lambda_userFX2 <- lambdas[which.min(rmses)]

pen_user_effects2 <- train %>% left_join(pen_movie_effects2, by = "movieId") %>% group_by(userId) %>% summarize(pen_user_effect = sum(rating - overall_ave - pen_movie_effect)/(n()+best_lambda_movieFX2))

#add effects to test data
meaningful_effects2 <- meaningful_effects2 %>% left_join(pen_user_effects2, by = "userId")

#confirm RMSE
cat("RMSE =", RMSE(overall_ave + meaningful_effects2$pen_movie_effect + meaningful_effects2$pen_user_effect, test$rating), "for pen. movie effect  + pen. movie-adjusted user effect.\n")
```

This produced an RMSE slightly smaller than that produced by the unpenalized movie effects, unpenalized adjuster user effects, and movie count effects combined from the older model, and it did it with without even including the residual movie count effects. Note that in this case, the lambdas were tuned a little more finely, and value of 1.5 was used for the movie effect, while a value of 5 was still used for the user effect. 

Given that the residual movie count effects are expected to be correlated to the penalized movie effects, it was investigated to see if an effect remained by graphing residuals of this new model vs the log-transformed movie rating counts:

```{r re-run of meaningful predictors part 2, echo = FALSE, fig.height = 3}
#check for residual movie count effects
train %>% left_join(pen_movie_effects2, by = "movieId") %>% left_join(pen_user_effects2, by = "userId") %>% group_by(movieId) %>% summarize(resid_movie_effect = mean(rating - overall_ave - pen_movie_effect - pen_user_effect), log_ratings_count = log(n())) %>% ggplot(aes(x=log_ratings_count,y=resid_movie_effect)) + geom_point(alpha = 0.2) + geom_smooth()
```

As expected, the residual effect was found to be significantly, if not entirely, reduced. A model was fit anyway to see if it would help. The resulting model details are shown below:

```{r re-run meaningful predictors part 3, echo = FALSE}
movie_counts2 <- train %>% left_join(pen_movie_effects2, by = "movieId") %>% left_join(pen_user_effects2, by = "userId") %>% group_by(movieId) %>% summarize(resid_movie_effect = mean(rating - overall_ave - pen_movie_effect - pen_user_effect), log_ratings_count = log(n()))

fit_count2 <- lm(resid_movie_effect~log_ratings_count, data = movie_counts2)
summary(fit_count2)
```

This still showed statistically significant, albeit smaller, coefficients, so this model was used to further predict the ratings of the test set, and the resulting RMSE was calculated:

```{r re-run meaningful predictors part 4, echo = FALSE}
#predict residual movie count effects of the test set
temp <- test %>% left_join(pen_movie_effects2, by = "movieId") %>% left_join(pen_user_effects2, by = "userId") %>% left_join(movie_counts2, by = "movieId") %>% select(-resid_movie_effect) %>% mutate(movie_ct_effect = predict(fit_count2,.))

#calculate new RMSE based on all effects plus new movie count effect
preds <- overall_ave + temp$pen_movie_effect + temp$pen_user_effect + temp$movie_ct_effect
cat("RMSE =",RMSE(preds,test$rating),"for pen. movie effect  + pen. movie-adjusted user effect + resid. movie count effect.","\n")
```

This actually increased the RMSE on the test set for this second model, indicating this last predictor overfit the training data. Reverting back to the model fit with only two predictors, however, gives a slightly higher RMSE than our original model involving all three predictors, indicating that the combination of an unpenalized movie effect and a linear-fit movie count effect, is better than a penalized movie effect on its own, at least with this data set. So these results were not added to the results table, and the original three-feature model was carried forward.

```{r leave only go-forward model, include = FALSE}
rm(fit_count2, meaningful_effects2, movie_counts2, adj_movie_effects, pen_movie_effects2, pen_user_effects2, user_effects, temp, adj_user_effects)

movie_ct_effects <- movie_counts[,-2] %>% mutate(movie_ct_effect = predict(fit_count, data = .)) %>% select(-log_ratings_count)

meaningful_effects <- meaningful_effects %>% select(-user_effect)

names(meaningful_effects)[which(names(meaningful_effects)=="pen_user_effect")] <- "pen_adj_user_effect"

col_order <- c("userId", "movieId", "rating", "timestamp", "title", "movie_effect", "pen_adj_user_effect", "movie_ct_effect")
meaningful_effects <- meaningful_effects[, ..col_order]
rm(col_order)

names(pen_adj_user_effects)[which(names(pen_adj_user_effects)=="pen_user_effect")] <- "pen_adj_user_effect"
```

### Temporal Effects

It was also questioned whether there was a temporal effect on user ratings - if a user tended to increase or decrease their mean rating over time.  This is likely similar to user count data, but slightly different, as it accounts for time and order rather than just overall quantity. The timestamp of each rating was analyzed and compared against other ratings from each user. Users were sampled with varying rating counts, and their ratings over time were plotted. The results for all sampled users are shown below:

```{r temporal effects of sampled users, echo=FALSE, fig.height=2.5}
#sample a subset of users with varying numbers of ratings recorded
sampled_rating_counts <- edx %>% group_by(userId) %>% summarize(count = n()) %>% arrange(count) %>% slice(round(seq(from=100, to = max(userId), length.out = 20)))
#plot their ratings over time using the timestamp value
timeplots <- list()
for(i in 1:nrow(sampled_rating_counts)){
  temp <- edx %>% filter(userId == sampled_rating_counts$userId[i]) 
 timeplots[[i]] <- ggplot(temp,aes(x = timestamp, y = rating)) + geom_point() + geom_smooth()
  rm(temp)
}
grid.arrange(timeplots[[1]],timeplots[[2]],timeplots[[3]],ncol=3,heights = c(1,1))
grid.arrange(timeplots[[4]],timeplots[[5]],timeplots[[6]],ncol=3,heights = c(1,1))
grid.arrange(timeplots[[7]],timeplots[[8]],timeplots[[9]],ncol=3,heights = c(1,1))
grid.arrange(timeplots[[10]],timeplots[[11]],timeplots[[12]],ncol=3,heights = c(1,1))
grid.arrange(timeplots[[13]],timeplots[[14]],timeplots[[15]],ncol=3,heights = c(1,1))
grid.arrange(timeplots[[16]],timeplots[[17]],timeplots[[18]],ncol=3,heights = c(1,1))
```

As shown, no clear effects were observed on any of the samples. The time elapsed between each rating and the user's first rating was then evaluated. The range and distribution of the days since first rating for all observations in the training set are shown below:

```{r days elapsed since first rating 1, echo = FALSE, fig.height = 4}
first_rating_dates <- train %>% arrange(timestamp) %>% mutate(date = as_datetime(timestamp), day = round_date(date, unit = "day")) %>% group_by(userId) %>% summarize(first_rating_date = first(day))

temp <- left_join(train,first_rating_dates, by = "userId") %>% mutate(date = as_datetime(timestamp), day = round_date(date, unit = "day"), days_since_first = interval(first_rating_date,day) %>% as.period %/% days())

range(temp$days_since_first)
hist(temp$days_since_first, xlab = "Days Since First Rating", main = "")
```

As shown, the days elapsed since a user's first rating and each observation range between 0 and 4122 with a heavy concentration near zero. Removing the zeros and plotting on log scale helps with visualization a little:

```{r log transformed days elapsed, echo = FALSE, fig.height = 4}
log(temp$days_since_first[temp$days_since_first > 0]) %>% hist(xlab = "Log of Days Since First Rating", main = "")
```

A randomly sampled segment (to reduce graphing time) of the time elapsed data was plotted against rating in search of trends:

```{r ratings vs elapsed days plot, fig.height = 3, echo = FALSE}
temp %>% slice_sample(prop = 0.01) %>% ggplot(aes(x=days_since_first,y=rating)) + geom_point(alpha=0.2) + geom_smooth()
```

Still no real trends were observed, so the search for a temporal effect was abandoned.

### Genre Effects

Another attempt at prediction improvement was made using the genre information, which had been mostly unused up to this point, with the notable exception that the movie effects were likely to pick up a good amount of the genre effects, as movies of those genres would likely be rated more highly on average. The overall movie effects, however, were likely to also include factors such as expensive marketing budgets and popular actors or directors that would affect ratings in a way that genre alone would not. To ensure no genre effects that did make it into the predictions were double-counted, however, the analysis was focused on the residuals, that is the variability that was still remaining after the current best model's predictions were made.

First, it was necessary to confirm that a genre effect even exists. Grouping the effects of the individual movies by their genres and averaging each group indicates that it does. Below are the 20 most impactful genres according to the averaged effects of movies within them:

```{r most impactful genres, echo = FALSE}
movie_effects %>% left_join(train, by = "movieId") %>% 
  select(movie_effect,genres) %>% group_by(genres) %>% 
  summarize(genre_effect = mean(movie_effect)) %>% 
  arrange(-abs(genre_effect)) %>% print(n=20)
```

It appeared that there were a lot of Horror, Action, and Sci-Fi themes in the groups that had significantly lower than average ratings.

Since the genres are bundled together in one column however, there were 797 genres recorded, each a unique blend of various other genres. Even with a data set this large, overfitting was a concern with that many predictors, so isolation of the genres that made up these combinations was attempted by separating the strings, and it was found that there were actually only 20 unique genres which made up the combinations:

```{r unique genres, echo = FALSE}
unique_genres <- edx$genres %>% str_split("\\|") %>% unlist() %>% unique()
unique_genres
```

The relative frequency of these genres was then estimated by sampling 100,000 ratings:

```{r genre samples, echo = FALSE}
sapply(X = unique_genres, FUN = str_detect, sample(edx$genres,100000)) %>% colSums()/100000
```

The most common genres reported were Drama and Comedy, while the most uncommon were IMAX and "(no genres listed)". In order to avoid building models off overly sparse data, genres that were reported in less than 2% of ratings were ignored. This resulted in removing "(no genres listed)", IMAX, Documentary, and Film Noir. The remaining genres are shown below in alphabetical order:

```{r genres of interest, echo = FALSE}
genres_of_interest <- unique_genres[!unique_genres %in% 
      c("Film-Noir", "Documentary", "IMAX", "(no genres listed)")]
genres_of_interest <- genres_of_interest[order(genres_of_interest)]
genres_of_interest
```

These genres were then coded into the training data as individual predictors using the following code:

```{r coding genres, echo = TRUE}
genre_combinations <- edx %>% group_by(genres) %>% summarize()

code.genres <- function(data){
    genres <- sapply(X = genres_of_interest, FUN = str_detect, data$genres)
    newdata <- cbind(data, genres)
    names(newdata)[which(names(newdata)=="Sci-Fi")] <- "SciFi"
    newdata
}

coded_genre_combinations <- code.genres(genre_combinations)

train_plus_features <- train %>% left_join(movie_effects, by = "movieId") %>% 
  left_join(pen_adj_user_effects, by = "userId") %>% 
  left_join(movie_ct_effects, by = "movieId") %>% 
  left_join(coded_genre_combinations, by = "genres")
```

Relationships between ratings and each of the individual genres were then sought out. First, to confirm a genre effect still existed at all, the centered ratings were regressed on the genres without any adjustments. The effects of each genre, ordered from most negative to most positive are shown below:

```{r ols genres vs ratings, echo = FALSE}
names(train_plus_features)[which(names(train_plus_features)=="Sci-Fi")] <- "SciFi"

genre_fx_on_ratings <- lm(rating - overall_ave ~ Action + Adventure + Animation + Children + Comedy + Crime + Drama + Fantasy + Horror + Musical + Mystery + Romance + SciFi + Thriller + War + Western, data = train_plus_features)

summary(genre_fx_on_ratings)$coefficients[-1,c(1,4)] %>% as.data.frame() %>% arrange(Estimate)
```

Note that the genre effects generally match those previously seen in terms of effect direction, and they are all statistically significant, indicating a genre effect indeed exists. The remaining residuals, instead of the centered ratings, were then regressed on the genres to see if these genre effects could actually improve the model. The results of this regression are shown below:

```{r ols genres vs residuals, echo = FALSE}
genre_fx_on_resids <- lm(rating - overall_ave - movie_effect - pen_adj_user_effect - movie_ct_effect ~ Action + Adventure + Animation + Children + Comedy + Crime + Drama + Fantasy + Horror + Musical + Mystery + Romance + SciFi + Thriller + War + Western, data = train_plus_features)

summary(genre_fx_on_resids)$coefficients[-1,c(1,4)] %>% as.data.frame() %>% arrange(Estimate)
```

As shown, removing the previously identified effects drove the detected genre effects effectively to zero and eliminated any statistical significance, meaning that any genre effects that existed across the full data set are already captured by the effects previously calculated. This was not surprising. It was expected that the movie effects in particular would pick up the majority of the genre effects, as a movie of a popular genre, such as war or drama, would be expected to receive higher average ratings when all users are considered. 

```{r clean up overall genre effects, echo = FALSE}
rm(genre_fx_on_ratings,genre_fx_on_resids)
```

What the overall movie effects would not take into account, however, are any particular user affinities for specific genres that deviate from normal preferences. For example, if a particular user loves horror movies, the normal adjustment for the horror genre would incorrectly drive down the predicted rating, and the overall movie effect would, on average, do the same, since most people have been shown to dislike horror movies. To account for these individual preferences, a series of user-specific genre model fits were attempted. This was attempted first on a single user, specifically user 3817, who had a high number of ratings in both the training and test data sets -- 3383 and 350, respectively, to be exact. The results of the model fit for this user are shown below:

```{r user 3817 genre effects, echo = FALSE}
user <- 3817

genre_fx_on_resids_solo_user <- lm(rating - overall_ave - movie_effect - pen_adj_user_effect - movie_ct_effect ~ Action + Adventure + Animation + Children + Comedy + Crime + Drama + Fantasy + Horror + Musical + Mystery + Romance + SciFi + Thriller + War + Western, data = filter(train_plus_features, userId == user))

summary(genre_fx_on_resids_solo_user)$coefficients[-1,c(1,4)] %>% as.data.frame() %>% arrange(Estimate)
```

Analyzing the residuals for user 3817 alone, a user-specific genre effect indeed emerges. This user appears to particularly like horror movies, with an average rating increase of 0.25 stars and a p-value of almost zero. They seem to also significantly dislike animation, romance, and drama. It was then tested if the dot product of this effect vector and each movie's genre vector, otherwise thought of as the overall user-specific genre effect for each movie's genre combination, would improve the RMSE for this user. The results of this test are shown below:

```{r test rmse user genre effects user 3817, echo = FALSE}
test_plus_features <- test %>% left_join(movie_effects, by = "movieId") %>% left_join(pen_adj_user_effects, by = "userId") %>% left_join(movie_ct_effects, by = "movieId") %>% left_join(coded_genre_combinations, by = "genres")

user_genre_effects_solo_user <- summary(genre_fx_on_resids_solo_user)$coefficients[-1,c(1,4)] %>% as.data.frame() %>% select(Estimate)

user_genre_effects <- data.frame(userId = user, t(user_genre_effects_solo_user))

test_plus_features_solo_user <- test_plus_features %>% filter(userId == user) %>% left_join(user_genre_effects, by = "userId")

user_genre_movie_effects_solo_user <- test_plus_features_solo_user %>% mutate(user_genre_movie_effect = Action*ActionTRUE + Adventure*AdventureTRUE + Animation*AnimationTRUE + Children*ChildrenTRUE + Comedy*ComedyTRUE + Crime*CrimeTRUE + Drama*DramaTRUE + Fantasy*FantasyTRUE + Horror*HorrorTRUE + Musical*MusicalTRUE + Mystery*MysteryTRUE + Romance*RomanceTRUE + SciFi*SciFiTRUE + Thriller*ThrillerTRUE + War*WarTRUE + Western*WesternTRUE) %>% select(user_genre_movie_effect)

new_fx_solo_user <- test_plus_features_solo_user %>% cbind(user_genre_movie_effects_solo_user) %>% select(userId, movieId, rating, title, genres, movie_effect, pen_adj_user_effect, movie_ct_effect, user_genre_movie_effect)

#check old RMSE for this user
cat("Old RMSE for user 3817:\n")
RMSE(overall_ave + new_fx_solo_user$movie_effect + new_fx_solo_user$pen_adj_user_effect + new_fx_solo_user$movie_ct_effect, new_fx_solo_user$rating)

#check new RMSE for this user
cat("New RMSE using user-specific genre effects:\n")
RMSE(overall_ave + new_fx_solo_user$movie_effect + new_fx_solo_user$pen_adj_user_effect + new_fx_solo_user$movie_ct_effect + new_fx_solo_user$user_genre_movie_effect, new_fx_solo_user$rating)
```

And indeed an improvement was found. However, many of these coefficients were not statistically significant, and this is even more likely to happen as this technique is applied to users with fewer ratings, so regularization with lasso regression was applied, which should drive insignificant coefficients to zero and leave only user-specific genre effects that meaningfully improve test RMSE, as determined by cross-validation. Lasso essentially takes the objective function that is minimized during ordinary least squares (OLS) and adds a penalty term (lambda) which is multiplied by absolute value of the coefficients in the objective function. As a result, the function pays a "penalty" for increasing the coefficient size and will only retain that increase if the benefit of the reduced loss function outweighs the damage caused by the penalty. Lambda is again in this case a tuning parameter that can take on any value ranging from 0, which would mean no penalty and therefore be identical to OLS, to infinite, in which case all coefficients would go to zero and the model would simply be the intercept. Here a range of lambdas from 0.0001 to 10 was tested.

```{r grid, echo = FALSE}
grid = 10^seq(-4,1,length=100)
```

The lasso-regression results for user 3817 are shown below:

```{r lasso user 3817, echo = FALSE}
#first make sure lasso yields same results at lambda = 0
cat("Lasso regression coefficients with lambda = 0 (same as ordinary least squares regression):\n")
out <- glmnet(x=as.matrix(filter(train_plus_features, userId == user)[,10:25]),
              y=as.matrix(filter(train_plus_features, userId == user)[,3]-overall_ave-rowSums(filter(train_plus_features, userId == user)[,c(7,8,9)])),alpha=1)
predict(out, type = "coefficients",s=0)[2:17,]

#then find best lambda
cv.out <- cv.glmnet(x=as.matrix(filter(train_plus_features, userId == user)[,10:25]),
                    y=as.matrix(filter(train_plus_features, userId == user)[,3]-overall_ave-rowSums(filter(train_plus_features, userId == user)[,c(7,8,9)])),alpha=1, lambda = grid, nfolds = 5)
bestlam <- cv.out$lambda.min
cat("Best lambda as determined by cross-validation:\n")
bestlam

#find coefficients with best lambda
cat("Lasso regression coefficients with optimized lambda:\n")
out <- glmnet(x=as.matrix(filter(train_plus_features, userId == user)[,10:25]),
              y=as.matrix(filter(train_plus_features, userId == user)[,3]-overall_ave-rowSums(filter(train_plus_features, userId == user)[,c(7,8,9)])),alpha=1)
predict(out, type = "coefficients",s=bestlam)[2:17,]
```

And it was found that the coefficients were similar but slightly reduced in magnitude at the optimized lambda. This is the result of regularization on a large data set. If the same regularization function were run on a user with significantly fewer ratings recorded, more shrinkage would be expected. User 54364, for example, had only 150 ratings in the test set and 1366 in the training set. Lasso regression on this user yields:

```{r lasso user 54364, echo = FALSE}
#set user ID
user <- 54364

#find best lambda
grid = 10^seq(4,-4,length=100)
cv.out <- cv.glmnet(x=as.matrix(filter(train_plus_features, userId == user)[,10:25]),
                    y=as.matrix(filter(train_plus_features, userId == user)[,3]-overall_ave-rowSums(filter(train_plus_features, userId == user)[,c(7,8,9)])),alpha=1, lambda = grid, nfolds = 5)
bestlam <- cv.out$lambda.min

#find coefficients with best lambda
out <- glmnet(x=as.matrix(filter(train_plus_features, userId == user)[,10:25]),
              y=as.matrix(filter(train_plus_features, userId == user)[,3]-overall_ave-rowSums(filter(train_plus_features, userId == user)[,c(7,8,9)])),alpha=1)
predict(out, type = "coefficients",s=bestlam)[2:17,]
```

In this case, many genre effects were driven entirely to zero, and the effects for Mystery (positive), Musical (negative), and Action (negative) were kept quite large. Looking at the ordinary linear regression for this user helps explain why:

```{r line reg for user 54364, echo = FALSE}
user <- 54364

genre_fx_on_resids_solo_user <- lm(rating - overall_ave - movie_effect - pen_adj_user_effect - movie_ct_effect ~ Action + Adventure + Animation + Children + Comedy + Crime + Drama + Fantasy + Horror + Musical + Mystery + Romance + SciFi + Thriller + War + Western, data = filter(train_plus_features, userId == user))

summary(genre_fx_on_resids_solo_user)$coefficients[-1,c(1,4)] %>% as.data.frame() %>% arrange(Estimate)
```

As expected, with ordinary least squares, Musical, Action, and Mystery all show significantly nonzero coefficients with relatively small p-values. Comparatively, the highly shrunken coefficients were very close to zero and had large p-values (thus were less significant). 

Note: There were other genres that also have relatively insignificant coefficients and were not pushed to zero. Because the method here involves cross-validation, the fits are random, and indeed if the analysis is run multiple times (not shown here), some of the other genres are set to zero as well. In all runs attempted though, the insignificant coefficients are drastically reduced in magnitude.

To demonstrate an extreme case, this method was tested on user 62, who had only 22 ratings in the training set and 4 ratings in the test set. The unpenalized linear regression coefficients for this user are shown below:

```{r line reg for user 62, echo = FALSE}
user <- 62

genre_fx_on_resids_solo_user <- lm(rating - overall_ave - movie_effect - pen_adj_user_effect - movie_ct_effect ~ Action + Adventure + Animation + Children + Comedy + Crime + Drama + Fantasy + Horror + Musical + Mystery + Romance + SciFi + Thriller + War + Western, data = filter(train_plus_features, userId == user))

summary(genre_fx_on_resids_solo_user)$coefficients[-1,c(1,4)] %>% as.data.frame() %>% arrange(Estimate)
```

Note that some coefficients deviated far from zero, but all had high p-values. The most significant effects seemed to be for War movies, which this user apparently really liked. The results of the lasso regression on this user are:

```{r lasso user 62, echo = FALSE}
#set user ID
user <- 62

#find best lambda
grid = 10^seq(4,-4,length=100)
cv.out <- cv.glmnet(x=as.matrix(filter(train_plus_features, userId == user)[,10:25]),
                    y=as.matrix(filter(train_plus_features, userId == user)[,3]-overall_ave-rowSums(filter(train_plus_features, userId == user)[,c(7,8,9)])),alpha=1, lambda = grid, nfolds = 5)
bestlam <- cv.out$lambda.min
bestlam

#find coefficients with best lambda
out <- glmnet(x=as.matrix(filter(train_plus_features, userId == user)[,10:25]),
              y=as.matrix(filter(train_plus_features, userId == user)[,3]-overall_ave-rowSums(filter(train_plus_features, userId == user)[,c(7,8,9)])),alpha=1)
predict(out, type = "coefficients",s=bestlam)[2:17,]
```

As expected, nearly all coefficients were set to zero. Among the exceptions was indeed the War genre, which had the highest coefficient of any genre. It was still significantly reduced from the OLS estimate, however, which should help prevent overfitting when dealing with such a small data set.

But the main question remained - do these penalized regressions lead to an improvement in RMSE on the test data? These fits were tested on the sampled users' test data to see. User 3817 was tested first, whose lasso fits were nearly identical to OLS fits, so a similar result was expected:

```{r lasso RMSE test user 3817, echo = FALSE}
#set user ID
user <- 3817

#find best lambda
cv.out <- cv.glmnet(x=as.matrix(filter(train_plus_features, userId == user)[,10:25]),
                    y=as.matrix(filter(train_plus_features, userId == user)[,3]-overall_ave-rowSums(filter(train_plus_features, userId == user)[,c(7,8,9)])),alpha=1, lambda = grid, nfolds = 5)
bestlam <- cv.out$lambda.min
cat("Best lambda:\n")
bestlam

#find coefficients with best lambda
out <- glmnet(x=as.matrix(filter(train_plus_features, userId == user)[,10:25]),
              y=as.matrix(filter(train_plus_features, userId == user)[,3]-overall_ave-rowSums(filter(train_plus_features, userId == user)[,c(7,8,9)])),alpha=1)

user_genre_effects_solo_user <- predict(out, type = "coefficients", s=bestlam)[2:17,]

user_genre_effects <- data.frame(userId = user, t(user_genre_effects_solo_user))

test_plus_features_solo_user <- test_plus_features %>% filter(userId == user) %>% left_join(user_genre_effects, by = "userId")

user_genre_movie_effects_solo_user <- test_plus_features_solo_user %>% mutate(user_genre_movie_effect = Action.x*Action.y + Adventure.x*Adventure.y + Animation.x*Animation.y + Children.x*Children.y + Comedy.x*Comedy.y + Crime.x*Crime.y + Drama.x*Drama.y + Fantasy.x*Fantasy.y + Horror.x*Horror.y + Musical.x*Musical.y + Mystery.x*Mystery.y + Romance.x*Romance.y + SciFi.x*SciFi.y + Thriller.x*Thriller.y + War.x*War.y + Western.x*Western.y) %>% select(user_genre_movie_effect)

new_fx_solo_user <- test_plus_features_solo_user %>% cbind(user_genre_movie_effects_solo_user) %>% select(userId, movieId, rating, title, genres, movie_effect, pen_adj_user_effect, movie_ct_effect, user_genre_movie_effect)

#check old RMSE for this user
cat("Old RMSE for user",user,":\n")
RMSE(overall_ave + new_fx_solo_user$movie_effect + new_fx_solo_user$pen_adj_user_effect + new_fx_solo_user$movie_ct_effect, new_fx_solo_user$rating)

#check new RMSE for this user
cat("New RMSE for user",user,"using user-specific genre effects:","\n")
RMSE(overall_ave + new_fx_solo_user$movie_effect + new_fx_solo_user$pen_adj_user_effect + new_fx_solo_user$movie_ct_effect + new_fx_solo_user$user_genre_movie_effect, new_fx_solo_user$rating)
```

And a similar improvement to what was previously shown was found. Similar results were then also found for the other two users:

```{r lasso RMSE test user 54364, echo = FALSE}
#set user ID
user <- 54364

#find best lambda
cv.out <- cv.glmnet(x=as.matrix(filter(train_plus_features, userId == user)[,10:25]),
                    y=as.matrix(filter(train_plus_features, userId == user)[,3]-overall_ave-rowSums(filter(train_plus_features, userId == user)[,c(7,8,9)])),alpha=1, lambda = grid, nfolds = 5)
bestlam <- cv.out$lambda.min
cat("Best lambda:\n")
bestlam

#find coefficients with best lambda
out <- glmnet(x=as.matrix(filter(train_plus_features, userId == user)[,10:25]),
              y=as.matrix(filter(train_plus_features, userId == user)[,3]-overall_ave-rowSums(filter(train_plus_features, userId == user)[,c(7,8,9)])),alpha=1)

user_genre_effects_solo_user <- predict(out, type = "coefficients", s=bestlam)[2:17,]

user_genre_effects <- data.frame(userId = user, t(user_genre_effects_solo_user))

test_plus_features_solo_user <- test_plus_features %>% filter(userId == user) %>% left_join(user_genre_effects, by = "userId")

user_genre_movie_effects_solo_user <- test_plus_features_solo_user %>% mutate(user_genre_movie_effect = Action.x*Action.y + Adventure.x*Adventure.y + Animation.x*Animation.y + Children.x*Children.y + Comedy.x*Comedy.y + Crime.x*Crime.y + Drama.x*Drama.y + Fantasy.x*Fantasy.y + Horror.x*Horror.y + Musical.x*Musical.y + Mystery.x*Mystery.y + Romance.x*Romance.y + SciFi.x*SciFi.y + Thriller.x*Thriller.y + War.x*War.y + Western.x*Western.y) %>% select(user_genre_movie_effect)

new_fx_solo_user <- test_plus_features_solo_user %>% cbind(user_genre_movie_effects_solo_user) %>% select(userId, movieId, rating, title, genres, movie_effect, pen_adj_user_effect, movie_ct_effect, user_genre_movie_effect)

#check old RMSE for this user
cat("Old RMSE for user",user,":\n")
RMSE(overall_ave + new_fx_solo_user$movie_effect + new_fx_solo_user$pen_adj_user_effect + new_fx_solo_user$movie_ct_effect, new_fx_solo_user$rating)

#check new RMSE for this user
cat("New RMSE for user",user,"using user-specific genre effects:","\n")
RMSE(overall_ave + new_fx_solo_user$movie_effect + new_fx_solo_user$pen_adj_user_effect + new_fx_solo_user$movie_ct_effect + new_fx_solo_user$user_genre_movie_effect, new_fx_solo_user$rating)
```

```{r lasso RMSE test user 62, echo = FALSE}
#set user ID
user <- 62

#find best lambda
cv.out <- cv.glmnet(x=as.matrix(filter(train_plus_features, userId == user)[,10:25]),
                    y=as.matrix(filter(train_plus_features, userId == user)[,3]-overall_ave-rowSums(filter(train_plus_features, userId == user)[,c(7,8,9)])),alpha=1, lambda = grid, nfolds = 5)
bestlam <- cv.out$lambda.min
cat("Best lambda:\n")
bestlam

#find coefficients with best lambda
out <- glmnet(x=as.matrix(filter(train_plus_features, userId == user)[,10:25]),
              y=as.matrix(filter(train_plus_features, userId == user)[,3]-overall_ave-rowSums(filter(train_plus_features, userId == user)[,c(7,8,9)])),alpha=1)

user_genre_effects_solo_user <- predict(out, type = "coefficients", s=bestlam)[2:17,]

user_genre_effects <- data.frame(userId = user, t(user_genre_effects_solo_user))

test_plus_features_solo_user <- test_plus_features %>% filter(userId == user) %>% left_join(user_genre_effects, by = "userId")

user_genre_movie_effects_solo_user <- test_plus_features_solo_user %>% mutate(user_genre_movie_effect = Action.x*Action.y + Adventure.x*Adventure.y + Animation.x*Animation.y + Children.x*Children.y + Comedy.x*Comedy.y + Crime.x*Crime.y + Drama.x*Drama.y + Fantasy.x*Fantasy.y + Horror.x*Horror.y + Musical.x*Musical.y + Mystery.x*Mystery.y + Romance.x*Romance.y + SciFi.x*SciFi.y + Thriller.x*Thriller.y + War.x*War.y + Western.x*Western.y) %>% select(user_genre_movie_effect)

new_fx_solo_user <- test_plus_features_solo_user %>% cbind(user_genre_movie_effects_solo_user) %>% select(userId, movieId, rating, title, genres, movie_effect, pen_adj_user_effect, movie_ct_effect, user_genre_movie_effect)

#check old RMSE for this user
cat("Old RMSE for user",user,":\n")
RMSE(overall_ave + new_fx_solo_user$movie_effect + new_fx_solo_user$pen_adj_user_effect + new_fx_solo_user$movie_ct_effect, new_fx_solo_user$rating)

#check new RMSE for this user
cat("New RMSE for user",user,"using user-specific genre effects:","\n")
RMSE(overall_ave + new_fx_solo_user$movie_effect + new_fx_solo_user$pen_adj_user_effect + new_fx_solo_user$movie_ct_effect + new_fx_solo_user$user_genre_movie_effect, new_fx_solo_user$rating)
```

In all three cases the test RMSE was improved. It's worth noting as well that the optimal lambda increased as the size of the data set decreased, meaning these smaller data sets were penalized more, as was intended. This provided confidence in expanding this methodology to the entire data set. The function was then applied to all users using the following code:

```{r find lasso genre effects across all users, echo = TRUE}
#create data frame to store all user genre effects
names <- colnames(user_genre_effects)
user_genre_effects <- data.frame(matrix(nrow = 1, ncol = 17, data = 0))
colnames(user_genre_effects) <- names
rm(names)

#identify all users with variance >0
users_lasso <- train %>% group_by(userId) %>% 
  summarize(var = var(rating)) %>% filter(var>0)

#calculate effects for each user and add to data frame
#i=1 - commented out - used only for progress report during dev

for(user in users_lasso$userId){
  #find best lambda
  cv.out <- cv.glmnet(x=as.matrix(filter(train_plus_features, userId == user)[,10:25]),
                      y=as.matrix(filter(train_plus_features, userId ==user)[,3] - overall_ave - 
                                    rowSums(filter(train_plus_features, userId == user)[,c(7,8,9)])),alpha=1, lambda = grid, nfolds = 5)
  bestlam <- cv.out$lambda.min

  #find coefficients with best lambda
  out <- glmnet(x=as.matrix(filter(train_plus_features, userId == user)[,10:25]),
                y=as.matrix(filter(train_plus_features, userId == user)[,3] - overall_ave - 
                              rowSums(filter(train_plus_features, userId == user)[,c(7,8,9)])),alpha=1)

  user_genre_effects_solo_user_vert <- predict(out, type = "coefficients", s=bestlam)[2:17,]

  user_genre_effects_solo_user_horiz <- data.frame(userId = user,
                                                   t(user_genre_effects_solo_user_vert))
  
  user_genre_effects <- rbind(user_genre_effects,user_genre_effects_solo_user_horiz)
  
  #report progress of loop during dev
  #cat(i*100/nrow(users_lasso),"%","\n")
  #i=i+1
}

#merge user-specific genre effects with observations
test_plus_features <- test_plus_features %>% 
  left_join(user_genre_effects, by = "userId")

#replace NAs with zeroes
test_plus_features[is.na(test_plus_features)] <- 0

#calculate dot products
test_plus_features <- test_plus_features %>% 
  mutate(user_genre_movie_effect = Action.x*Action.y + Adventure.x*Adventure.y +
           Animation.x*Animation.y + Children.x*Children.y + Comedy.x*Comedy.y +
           Crime.x*Crime.y + Drama.x*Drama.y + Fantasy.x*Fantasy.y + Horror.x*Horror.y +
           Musical.x*Musical.y + Mystery.x*Mystery.y + Romance.x*Romance.y +
           SciFi.x*SciFi.y + Thriller.x*Thriller.y + War.x*War.y + Western.x*Western.y)
```

This script took approximately 10 hours to run on the full data set. One of the sample users was first checked to ensure the analysis was done correctly. The data for the first rating from user 3817 is shown below. Note that each genre.x represents whether the movie belongs to the genre, while each genre.y represents the user's affinity for the genre. The user_genre_movie_effect is the dot product of both of these vectors.

```{r lasso check user 3817 values, echo = FALSE}
#user 3817 value check
test_plus_features %>% filter(userId == 3817) %>% first() %>% t()
```

The test RMSE for this user was assessed using the old and new models, and approximately the same improvement as previously calculated was confirmed, shown below. (Recall it may not be identical, as there is some randomness in the tuning of lambda.)

```{r lasso check sample user RMSEs, echo = FALSE}
cat("Old RMSE for user 3817:\n")
test_plus_features %>% filter(userId == 3817) %>% summarize(rmse = sqrt(mean((rating - overall_ave - movie_effect - pen_adj_user_effect - movie_ct_effect)^2)))
cat("New RMSE for user 3817:\n")
test_plus_features %>% filter(userId == 3817) %>% summarize(rmse = sqrt(mean((rating - overall_ave - movie_effect - pen_adj_user_effect - movie_ct_effect - user_genre_movie_effect)^2)))
```

The resulting RMSE for the full data set was then calculated, and indeed a small but meaningful improvement was observed:

```{r lasso rmse check on full set, echo = FALSE}
cat("\nOld RMSE for all users:\n")
test_plus_features %>% summarize(rmse = sqrt(mean((rating - overall_ave - movie_effect - pen_adj_user_effect - movie_ct_effect)^2)))
cat("New RMSE for all users:\n")
test_plus_features %>% summarize(rmse = sqrt(mean((rating - overall_ave - movie_effect - pen_adj_user_effect - movie_ct_effect - user_genre_movie_effect)^2)))
```

This value was added to the results table:

```{r results update 6, echo = FALSE}
rmse_results <- rbind(rmse_results,tibble(Method = "Ave + MovieFX + Pen_Adj_UserFX + Movie_Ct_FX + User_Genre_Movie_FX",RMSE = RMSE(overall_ave + test_plus_features$movie_ct_effect + test_plus_features$movie_effect + test_plus_features$pen_adj_user_effect + test_plus_features$user_genre_movie_effect, test$rating)))
rmse_results
```

It was clear, however, that although the RMSE was reduced overall, it actually increased for some users. Below are outcomes for 30 randomly selected users from the test data set, shown alongside their rating counts and variances. 

```{r new rmses for 30 sampled users, echo = FALSE}
test_plus_features %>% group_by(userId) %>% summarize(count = n(), variance = var(rating), rmse_old = sqrt(mean((rating - overall_ave - movie_effect - pen_adj_user_effect - movie_ct_effect - user_genre_movie_effect)^2)), rmse_new = sqrt(mean((rating - overall_ave - movie_effect - pen_adj_user_effect - movie_ct_effect)^2))) %>% slice_sample(n=30) %>% print(n=30)
```

As shown, predictions were actually worse for some users when the new model was applied. There was no immediate, obvious indication as to which users posed this issue, and since the analysis time was so extreme, further fine-tuning of this model component was not attempted, though clearly there is room for improvement. The user-specific genre effects calculated here were the final additions made to the linear model. 

### Matrix Factorization

The final method attempted was matrix factorization. This method considers the sparse matrix of users (rows) by movies (columns) and the ratings values that exist for each observation, which will be the only values in the matrix that are actually filled in. (In this case we have only have about 1% of ratings in the training data set out of all possible user-movie combinations.) The covariances between all users and all movies are analyzed in order to identify latent features that group movies into common categories and group users by their affinity for those categories. Matrix factorization requires a gradient descent algorithm to converge on pair of matrices P and Q that represent the user-feature loadings and the movie-feature loadings, respectively, and are able to approximately reconstruct the original matrix while filling in the missing gaps. The dot products of these P and Q matrices for any particular user-movie combination are what produce the predictions based on the interaction of features. The concept is somewhat similar to the user-specific genre effects described earlier, as it uses both a movie loading vector and a user loading vector to generate a prediction, though in this case the analysis results in many implicit features beyond just genres being considered, for example, movies that star Tom Hanks and the set of users who are die-hard Tom Hanks fans. (Even though this data is not explicitly defined in the data set, it can be identified using matrix factorization.)

For this model, the recoystem package was utilized along with the input and tuning parameters recommended at https://cran.r-project.org/web/packages/recosystem/readme/README.html. The resulting RMSEs from the recosystem matrix factorization algorithm, both from the gradient descent iterations on training data, and the final test RMSE, are shown below:

```{r matrix factorization, echo = FALSE}
train_reco <- data_memory(user_index = train$userId, item_index = train$movieId, rating = train$rating, index1 = TRUE)

test_reco <- data_memory(user_index = test$userId, item_index = test$movieId, rating = test$rating, index1 = TRUE)

r = Reco()

opts = r$tune(train_reco, opts = list(dim = c(10, 20, 30), 
              lrate = c(0.1, 0.2), costp_l1 = 0, costq_l1 = 0,
              nthread = 1, niter = 10))

r$train(train_reco, opts = c(opts$min, nthread = 1, niter = 10))

cat("Test RMSE from matrix factorization model:\n")
RMSE(r$predict(test_reco, out_memory()),test$rating)
```

This was the lowest RMSE of any model attempted so far, so it was added to our results table:

```{r results update 7, echo = FALSE}
rmse_results <- rbind(rmse_results,tibble(Method = "Matrix Factorization",RMSE = RMSE(r$predict(test_reco, out_memory()),test$rating)))
rmse_results
```

### Model Comparison and Further Adjustment

The distributions of the residuals left by the final linear model and the matrix factorization model were then compared:

```{r model residuals comparison, echo = FALSE, fig.height = 4}
preds_matfact <- r$predict(test_reco, out_memory())

preds_linear <- overall_ave + test_plus_features$movie_effect + test_plus_features$pen_adj_user_effect + test_plus_features$movie_ct_effect + test_plus_features$user_genre_movie_effect

data.frame(test$rating,preds_linear,preds_matfact) %>% mutate(resids_linear = test.rating - preds_linear, resids_matfact = test.rating - preds_matfact) %>% gather(resids_linear, resids_matfact, key = "model", value = "resid") %>% select(model, resid) %>% ggplot(aes(x=resid,col=model,fill=model)) + geom_density(alpha=0.2) + xlim(-3,3)
```

Clearly the distribution was wider for the linear model than the matrix factorization model, which was expected since the RMSE is higher. One unexpected finding, however, was that both distributions were right-shifted. With their centers slightly above zero, this meant that both models were underestimating the actual ratings. To see if a final adjustment could be made to account for this, the training data residual distributions were first plotted in order to confirm the trend existed there as well:

```{r training residual distributions, echo = FALSE, fig.height = 4}
train_plus_features <- train_plus_features %>% left_join(user_genre_effects, by = "userId")

train_plus_features[is.na(train_plus_features)] <- 0

resids_linear_train <- train_plus_features %>% mutate(user_genre_movie_effect = Action.x*Action.y + Adventure.x*Adventure.y + Animation.x*Animation.y + Children.x*Children.y + Comedy.x*Comedy.y + Crime.x*Crime.y + Drama.x*Drama.y + Fantasy.x*Fantasy.y + Horror.x*Horror.y + Musical.x*Musical.y + Mystery.x*Mystery.y + Romance.x*Romance.y + SciFi.x*SciFi.y + Thriller.x*Thriller.y + War.x*War.y + Western.x*Western.y, resids_linear_train = rating - overall_ave - movie_effect - pen_adj_user_effect - movie_ct_effect -user_genre_movie_effect) %>% select(resids_linear_train)

resids_matfact_train <- train$rating - r$predict(train_reco, out_memory())

data.frame(resids_linear_train,resids_matfact_train) %>% gather(key = "model", value = "resid") %>% select(model, resid) %>% ggplot(aes(x=resid,col=model,fill=model)) + geom_density(alpha=0.2) + xlim(-3,3)
```

And the same effect was indeed observed. While the reason for the shift was not investigated, it was hypothesized that the test RMSE could be further reduced by adding the average training data residual to each of the test predictions:

```{r adjusted matrix fact preds, echo = FALSE}
cat("Mean residual from matrix fact predictions on training data:\n")
mean(resids_matfact_train)

cat("Test RMSE from adjusted matrix factorization model:\n")
RMSE(r$predict(test_reco, out_memory())+mean(resids_matfact_train),test$rating)
```

This led to further reduction of the RMSE as expected, so this model, which was the final optimized model in this study, was added to the results table:

```{r results update 8, echo = FALSE}
rmse_results <- rbind(rmse_results,tibble(Method = "Residual-Adjusted Matrix Factorization",RMSE = RMSE(r$predict(test_reco, out_memory())+mean(resids_matfact_train),test$rating)))
rmse_results
```

## Results on Validation Data

The final test was to take the optimal model, which was the residual-adjusted matrix factorization model, train it on the full edX data set, and test it on the validation set, which up to this point had not been used at all for training or testing of any attempted models. The code to run this model and the RMSE produced by this evaluation is shown below:

```{r final validation, echo = TRUE}
#format edx data for reco package
edx_reco <- data_memory(user_index = edx$userId, item_index = edx$movieId, 
                        rating = edx$rating, index1 = TRUE)

#format validation data for reco package
validation_reco <- data_memory(user_index = validation$userId, 
                               item_index = validation$movieId, 
                               rating = validation$rating, index1 = TRUE)

#initialize reco object
r = Reco()

#set tuning parameters
opts = r$tune(edx_reco, opts = list(dim = c(10, 20, 30), 
              lrate = c(0.1, 0.2), costp_l1 = 0, costq_l1 = 0,
              nthread = 1, niter = 10))

#train the model
r$train(edx_reco, opts = c(opts$min, nthread = 1, niter = 10))

#calculate predictions and RMSE
cat("Validation RMSE from matrix factorization model:\n")
RMSE(r$predict(validation_reco, out_memory()),validation$rating)

#calculate residuals
resids_matfact_edx <- edx$rating - r$predict(edx_reco, out_memory())

#add residual mean to predictions for final model improvement
cat("Mean residual from matrix fact predictions on edx data:\n")
mean(resids_matfact_edx)

cat("Validation RMSE from adjusted matrix factorization model:\n")
RMSE(r$predict(validation_reco, out_memory())+mean(resids_matfact_edx),validation$rating)
```

As shown, the RMSE held up on the validation data, even came in slightly lower than on the test data set, providing confidence that useful model had been fit that can be used to predict movie ratings with a reasonable level of accuracy.

## Conclusions and Further Recommendations

While the models produced here were an excellent first pass, there are likely still improvements that can be made. For the linear model, temporal effects could be explored more closely, as it has been reported that these have been useful in other recommendation systems. The user-specific genre effects could also be better tuned. Lasso is only one of several regularization methods available, and clearly the version chosen left some users with worse RMSEs than they started with, so perhaps cut-offs could be established that determine which users are suitable candidates for calculating genre effects.

The use of the two models themselves could also be more carefully applied. Although not shown here, it was found that the linear model actually made better predictions than matrix factorization for almost 40% of the test observations. No obvious conditions that led to these situations were apparent based on graphing the model results against various features - they seemed to be distributed evenly throughout the data set. However, random forests may be useful in splitting the data to identify non-obvious groups that would benefit from one model over another and produce better results by combining the two models in an ensemble rather than only using the adjusted matrix factorization model on all data points.

Lastly, one method that has been shown to be successful in other recommendation algorithms but was not attempted at all here is collaborative filtering, in which a user is ranked in proximity against similar users, and movies against similar movies, and predictions are made solely based on the similarities between those groupings. This may lead to RMSE improvements when combined with or used instead of the linear and matrix factorization models demonstrated here.
